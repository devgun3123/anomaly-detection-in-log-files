{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 5306888191563489136\n",
       " xla_global_id: -1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.test.is_gpu_available()\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class Config:\n",
    "    PROJECT_DIR = os.getcwd()\n",
    "    DATA_DIR = os.getenv(\"DATA_DIR\", \"data/\")\n",
    "    RESULTS_DIR = os.getenv(\"RESULTS_DIR\", \"results/\")\n",
    "    MODELS_DIR = os.getenv(\"MODELS_DIR\", \"models/\")\n",
    "    CHECKPOINT_DIR = os.getenv(\"CHECKPOINT_DIR\", \"models/checkpoint/\")\n",
    "    LOGS_DIR = os.getenv(\"LOGS_DIR\", \"logs/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(X, y, num_s, num_e, ratio):\n",
    "    print('Stats:')\n",
    "    print(\"------------------------\")\n",
    "    print(\"------------------------\")\n",
    "    print(f'N(X) == N(y) == {len(y)}')\n",
    "    print(f'errs: {num_e}')\n",
    "    print(f'Clean data (N = {num_s}) ratio: {ratio}%')\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "X_train_dir = f\"{config.DATA_DIR}clean/\"\n",
    "y_train_dir = f\"{config.DATA_DIR}trans/\"\n",
    "X_test_dir = f\"{config.DATA_DIR}test/\"\n",
    "\n",
    "# Will notify if these values change\n",
    "max_encoder_seq_length = 81\n",
    "max_decoder_seq_length = 162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "# All of the characters and substring that would mark lines in the training data as \"faulty\"\n",
    "invalid_chars = set(\n",
    "    [\n",
    "        \":\",\n",
    "        \"+\",\n",
    "        \"#\",\n",
    "        \"@\",\n",
    "        \"Ö\",\n",
    "        \"á\",\n",
    "        \"ä\",\n",
    "        \"é\",\n",
    "        \"í\",\n",
    "        \"ñ\",\n",
    "        \"ó\",\n",
    "        \"ö\",\n",
    "        \"ú\",\n",
    "        \"ā\",\n",
    "        \"Ć\",\n",
    "        \"ć\",\n",
    "        \"ʻ\",\n",
    "        \"́\",\n",
    "        \"е\",\n",
    "        \"н\",\n",
    "        \"о\",\n",
    "        \"п\",\n",
    "        \"у\",\n",
    "        \"ш\",\n",
    "        \"ã\",\n",
    "        \"ï\",\n",
    "        \"ō\",\n",
    "        \"ū\",\n",
    "        \"ί\",\n",
    "        \"α\",\n",
    "        \"δ\",\n",
    "        \"ε\",\n",
    "        \"κ\",\n",
    "        \"ο\",\n",
    "        \"в\",\n",
    "        \"ὐ\",\n",
    "        chr(776),\n",
    "        \"ç\",\n",
    "        \"ē\",\n",
    "        \"D\",\n",
    "        \"O\",\n",
    "        \"T\",\n",
    "    ]\n",
    ")\n",
    "invalid_chars_X = set([\"(\", \")\", \"<\", \">\", \"_\", \",\"])\n",
    "invalid_markers = set([\"\\\\F\", \"TrueP\", \"\\\\x\", \"semantics_error\", \"Prog(\"])\n",
    "files_with_compound_preds = [20, 21, 15]\n",
    "\n",
    "\n",
    "def mark_if_faulty(line, file_idx, X=False):\n",
    "    if X and (\n",
    "        any((c in invalid_chars) for c in line)\n",
    "        or any((c in invalid_chars_X) for c in line)\n",
    "    ):\n",
    "        return \"syntax_error\"\n",
    "    # TODO: Refactor this hacky workaround\n",
    "    if line[0] == \"(\" and file_idx not in files_with_compound_preds:\n",
    "        return \"syntax_error\"\n",
    "    if any((m in line) for m in invalid_markers) or any(\n",
    "        (c in invalid_chars) for c in line\n",
    "    ):\n",
    "        return \"syntax_error\"\n",
    "    # Remove top-level parentheses from lambda expression\n",
    "    if line[0:4] == \"(exi\" and line[-1] == \")\":\n",
    "        line = line[1:-1]\n",
    "    if line[0:4] == \"(all\" and line[-1] == \")\":\n",
    "        line = line[1:-1]\n",
    "\n",
    "    return line\n",
    "\n",
    "\n",
    "def lines_from_file(direc, name, drop_punc=False, lower=True, drop_fullstop=True):\n",
    "    with open(direc + name) as f:\n",
    "        for l in f:\n",
    "            l = l.rstrip()\n",
    "            if drop_punc:\n",
    "                l = l.translate(table)\n",
    "            if lower:\n",
    "                l = l.lower()\n",
    "            if drop_fullstop and not drop_punc:\n",
    "                l = l[0:-1]\n",
    "            yield l\n",
    "\n",
    "\n",
    "def load_and_clean_data(start_idx=1, end_idx=17, skip_idx_list=None):\n",
    "    X, y = [], []\n",
    "\n",
    "    err = lambda x: x == \"syntax_error\"\n",
    "    X_name = lambda i: f\"concordance_{i}_clean.txt\"\n",
    "    y_name = lambda i: f\"concordance_{i}_clean.lam\"\n",
    "\n",
    "    # Load lines from files and mark those that are \"faulty\"\n",
    "    for i in range(start_idx, end_idx + 1):\n",
    "        if i in skip_idx_list:\n",
    "            continue\n",
    "\n",
    "        X = X + [\n",
    "            mark_if_faulty(line, i, True)\n",
    "            for line in lines_from_file(X_train_dir, X_name(i), drop_fullstop=True)\n",
    "        ]\n",
    "        y = y + [\n",
    "            mark_if_faulty(line, i)\n",
    "            for line in lines_from_file(\n",
    "                y_train_dir, y_name(i), lower=False, drop_fullstop=False\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Save \"faulty\" line indices\n",
    "    err_idx_X = [i1 for i1 in range(len(X)) if err(X[i1])]\n",
    "    err_idx_y = [j1 for j1 in range(len(X)) if err(y[j1])]\n",
    "\n",
    "    err_idx = set(err_idx_X).union(set(err_idx_y))\n",
    "    num_err = len(err_idx)\n",
    "    num_samples = len(y) - num_err\n",
    "    clean_ratio = 100 - ((num_err / len(y)) * 100)\n",
    "\n",
    "    # Show stats about training data\n",
    "    print_stats(X, y, num_samples, num_err, clean_ratio)\n",
    "\n",
    "    # Remove \"faulty\" lines\n",
    "    for index in sorted(list(err_idx), reverse=True):\n",
    "        del X[index]\n",
    "        del y[index]\n",
    "\n",
    "    return (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats:\n",
      "------------------------\n",
      "------------------------\n",
      "N(X) == N(y) == 40000\n",
      "errs: 0\n",
      "Clean data (N = 40000) ratio: 100.0%\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    *load_and_clean_data(1, 20, [ ]), test_size=0.25, random_state=4, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 30000\n",
      "Number of unique input tokens: 37\n",
      "Number of unique output tokens: 45\n",
      "WARNING: NEW Max sequence length for inputs: 293\n",
      "Dataset may be incompatible with older models.\n",
      "['\\t', '\\n', ' ', '&', '(', ')', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "for i in range(0, len(X_train)):\n",
    "    # SOS == '\\n'\n",
    "    # EOS == '\\t'\n",
    "    y_train[i] = \"\\t\" + y_train[i] + \"\\n\"\n",
    "\n",
    "    for char in X_train[i]:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in y_train[i]:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_X_len = max([len(txt) for txt in X_train])\n",
    "max_y_len = max([len(txt) for txt in y_train])\n",
    "\n",
    "print(\"Number of samples:\", len(X_train))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "\n",
    "if max_X_len > max_encoder_seq_length:\n",
    "    print(\"WARNING: NEW Max sequence length for inputs:\", max_X_len)\n",
    "    print(\"Dataset may be incompatible with older models.\")\n",
    "    max_encoder_seq_length = max_X_len\n",
    "\n",
    "if max_y_len > max_decoder_seq_length:\n",
    "    print(\"WARNING: NEW Max sequence length for outputs:\", max_y_len)\n",
    "    print(\"Dataset may be incompatible with older models.\")\n",
    "    max_decoder_seq_length = max_y_len\n",
    "\n",
    "print(target_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char to index\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(X_train), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(X_train), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(X_train), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "np.set_printoptions(threshold=10)\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(X_train, y_train)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
    "\n",
    "encoder_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(\n",
    "            name=\"W_a\",\n",
    "            shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.U_a = self.add_weight(\n",
    "            name=\"U_a\",\n",
    "            shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.V_a = self.add_weight(\n",
    "            name=\"V_a\",\n",
    "            shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        super(AttentionLayer, self).build(\n",
    "            input_shape\n",
    "        )  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print(\"encoder_out_seq>\", encoder_out_seq.shape)\n",
    "            print(\"decoder_out_seq>\", decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\"Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(\n",
    "                states, type(states)\n",
    "            )\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(\n",
    "                K.dot(inputs, self.U_a), 1\n",
    "            )  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print(\"Ua.h>\", U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "            if verbose:\n",
    "                print(\"Ws+Uh>\", Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"ei>\", e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(\n",
    "                states, type(states)\n",
    "            )\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print(\"ci>\", c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(\n",
    "            encoder_out_seq, axis=2\n",
    "        )  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step,\n",
    "            decoder_out_seq,\n",
    "            [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step,\n",
    "            e_outputs,\n",
    "            [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1])),\n",
    "        ]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    TimeDistributed,\n",
    "    Bidirectional,\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from livelossplot import PlotLossesKeras"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None, 37)]   0           []                               \n",
      "                                                                                                  \n",
      " bidirectional_encoder (Bidirec  [(None, None, 88),  28864       ['input_1[0][0]']                \n",
      " tional)                         (None, 44),                                                      \n",
      "                                 (None, 44),                                                      \n",
      "                                 (None, 44),                                                      \n",
      "                                 (None, 44)]                                                      \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, None, 45)]   0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 88)           0           ['bidirectional_encoder[0][1]',  \n",
      "                                                                  'bidirectional_encoder[0][3]']  \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 88)           0           ['bidirectional_encoder[0][2]',  \n",
      "                                                                  'bidirectional_encoder[0][4]']  \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 88),   47168       ['input_3[0][0]',                \n",
      "                                 (None, 88),                      'concatenate[0][0]',            \n",
      "                                 (None, 88)]                      'concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " attention_layer (AttentionLaye  ((None, None, 88),  15576       ['bidirectional_encoder[0][0]',  \n",
      " r)                              (None, None, None)               'decoder_lstm[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)     (None, None, 176)    0           ['decoder_lstm[0][0]',           \n",
      "                                                                  'attention_layer[0][0]']        \n",
      "                                                                                                  \n",
      " softmax_layer (Dense)          (None, None, 45)     7965        ['concat_layer[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 99,573\n",
      "Trainable params: 99,573\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 44\n",
    "batch_size = 48  # Batch size for training.\n",
    "epochs = 30  # Number of epochs to train for.\n",
    "recurrent_dropout_rate = 0.2\n",
    "simple_name = \"bilstm\"\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# Encoder LSTM\n",
    "encoder_lstm = Bidirectional(\n",
    "    LSTM(\n",
    "        latent_dim,\n",
    "        return_sequences=True,\n",
    "        return_state=True,\n",
    "        name=\"encoder_lstm\",\n",
    "        recurrent_dropout=recurrent_dropout_rate,\n",
    "    ),\n",
    "    name=\"bidirectional_encoder\",\n",
    ")\n",
    "(\n",
    "    encoder_out,\n",
    "    encoder_fwd_state_h,\n",
    "    encoder_fwd_state_c,\n",
    "    encoder_back_state_h,\n",
    "    encoder_back_state_c,\n",
    ") = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(\n",
    "    latent_dim * 2,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_dropout=recurrent_dropout_rate,\n",
    "    name=\"decoder_lstm\",\n",
    ")\n",
    "decoder_out, _, _ = decoder_lstm(\n",
    "    decoder_inputs,\n",
    "    initial_state=[\n",
    "        Concatenate(axis=-1)([encoder_fwd_state_h, encoder_back_state_h]),\n",
    "        Concatenate(axis=-1)([encoder_fwd_state_c, encoder_back_state_c]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name=\"attention_layer\")\n",
    "attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")(\n",
    "    [decoder_out, attn_out]\n",
    ")\n",
    "\n",
    "# Dense layer\n",
    "dense = Dense(num_decoder_tokens, activation=\"softmax\", name=\"softmax_layer\")\n",
    "decoder_pred = dense(decoder_concat_input)\n",
    "\n",
    "# Optimizer\n",
    "opt = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.0015,\n",
    "    rho=0.9,\n",
    "    momentum=0.0,\n",
    "    epsilon=1e-07,\n",
    "    centered=True,\n",
    "    name=\"RMSprop\",\n",
    ")\n",
    "\n",
    "# Full model\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "full_model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "full_model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint file 'models/checkpoint/weights-bilstm-N(30000)-44.best.hdf5' not found. Starting training from scratch.\n",
      "Epoch 1/30\n",
      " 20/563 [>.............................] - ETA: 30:32 - loss: 1.6992 - accuracy: 0.6235"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "cp_path = f\"{config.CHECKPOINT_DIR}weights-{simple_name}-N({len(X_train)})-{latent_dim}.best.hdf5\"\n",
    "\n",
    "# Check if the directory exists, create if it doesn't\n",
    "cp_dir = os.path.dirname(cp_path)\n",
    "if not os.path.exists(cp_dir):\n",
    "    os.makedirs(cp_dir)\n",
    "\n",
    "try:\n",
    "    # Check if the checkpoint file exists\n",
    "    if not os.path.exists(cp_path):\n",
    "        print(f\"Checkpoint file '{cp_path}' not found. Starting training from scratch.\")\n",
    "    else:\n",
    "        # Load weights from the checkpoint file\n",
    "        full_model.load_weights(cp_path)\n",
    "        print(f\"Checkpoint file '{cp_path}' loaded. Resuming training.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the checkpoint file: {str(e)}\")\n",
    "\n",
    "# Callbacks\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stop = EarlyStopping(patience=2, monitor=\"val_loss\", mode=\"min\", verbose=1)\n",
    "plot_loss = PlotLossesKeras()\n",
    "checkpoint = ModelCheckpoint(\n",
    "    cp_path, save_weights_only=True, verbose=1, monitor=\"val_loss\", save_best_only=True\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "history = full_model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stop, plot_loss, checkpoint],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Print progress\n",
    "for epoch, loss in enumerate(history.history['loss'], 1):\n",
    "    print(f\"Epoch {epoch}/{epochs} - Loss: {loss:.4f}\")\n",
    "\n",
    "for epoch, val_loss in enumerate(history.history['val_loss'], 1):\n",
    "    print(f\"Epoch {epoch}/{epochs} - Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"model accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.save(f\"{config.MODELS_DIR}{simple_name}-N({len(X_train)})-{latent_dim}.best.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log-nlp_39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
