DEFINE CLASS AttentionLayer(Layer):
    # This DEFINE CLASS implements Bahdanau attention.

DEFINE FUNCTION __init__(self, ** kwargs):
    super(AttentionLayer, self).__init__( ** kwargs)

DEFINE FUNCTION build(self, INPUT_shape):
    assert isinstance(INPUT_shape, list)

# Create a trainable weight variable FOR this layer.
SET self.W_a TO self.add_weight(
    name = "W_a",
    shape = tf.TensorShape((INPUT_shape[0][2], INPUT_shape[0][2])),
    initializer = "uniform",
    trainable = True,
)

SET self.U_a TO self.add_weight(
    name = "U_a",
    shape = tf.TensorShape((INPUT_shape[1][2], INPUT_shape[0][2])),
    initializer = "uniform",
    trainable = True,
)

SET self.V_a TO self.add_weight(
    name = "V_a",
    shape = tf.TensorShape((INPUT_shape[0][2], 1)),
    initializer = "uniform",
    trainable = True,
)

super(AttentionLayer, self).build(
    INPUT_shape
) # Be sure to call this at the end

DEFINE FUNCTION call(self, INPUTs, verbose = False):

    INPUTs: [encoder_output_sequence, decoder_output_sequence]
assert type(INPUTs) EQUALS list
SET encoder_out_seq, decoder_out_seq TO INPUTs

DEFINE FUNCTION energy_step(INPUTs, states):

    INPUTs: (batchsize * 1 * de_in_dim)
states: (batchsize * 1 * de_latent_dim)

SET assert_msg TO "States must be an iterable. Got {} of type {}".format(
    states, type(states)
)

assert isinstance(states, list) or isinstance(states, tuple), assert_msg

# Some parameters required FOR shaping tensors
SET en_seq_len, en_hidden TO encoder_out_seq.shape[1], encoder_out_seq.shape[2]
SET de_hidden TO INPUTs.shape[-1]

# Computing S.Wa where S=[s0, s1, ..., si]
# <= batch size * en_seq_len * latent_dim
SET W_a_dot_s TO K.dot(encoder_out_seq, self.W_a)

# Computing hj.Ua
SET U_a_dot_h TO K.expand_dims(
    K.dot(INPUTs, self.U_a), 1
) # <= batch_size, 1, latent_dim

# tanh(S.Wa + hj.Ua)
# <= batch_size * en_seq_len, latent_dim
SET Ws_plus_Uh TO K.tanh(W_a_dot_s + U_a_dot_h)
IF verbose:
    OUTPUT("Ws+Uh>", Ws_plus_Uh.shape)


# softmax(va.tanh(S.Wa + hj.Ua))
# <= batch_size, en_seq_len
SET e_i TO K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis = -1)
# <= batch_size, en_seq_len
SET e_i TO K.softmax(e_i)

RETURN e_i, [e_i]