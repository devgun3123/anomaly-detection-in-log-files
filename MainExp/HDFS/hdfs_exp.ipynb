{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.15\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    PROJECT_DIR = os.environ[\"PWD\"]\n",
    "    DATA_DIR = os.getenv(\"DATA_DIR\", \"data/\")\n",
    "    RESULTS_DIR = os.getenv(\"RESULTS_DIR\", \"results/\")\n",
    "    MODELS_DIR = os.getenv(\"MODELS_DIR\", \"models/\")\n",
    "    CHECKPOINT_DIR = os.getenv(\"CHECKPOINT_DIR\", \"models/checkpoint/\")\n",
    "    LOGS_DIR = os.getenv(\"LOGS_DIR\", \"logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(X, y, num_s, num_e, ratio):\n",
    "    print('Stats:')\n",
    "    print(\"------------------------\")\n",
    "    print(\"------------------------\")\n",
    "    print(f'N(X) == N(y) == {len(y)}')\n",
    "    print(f'errs: {num_e}')\n",
    "    print(f'Clean data (N = {num_s}) ratio: {ratio}%')\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "X_train_dir = f\"{config.DATA_DIR}clean/\"\n",
    "y_train_dir = f\"{config.DATA_DIR}trans/\"\n",
    "X_test_dir = f\"{config.DATA_DIR}test/\"\n",
    "\n",
    "# Will notify if these values change\n",
    "max_encoder_seq_length = 81\n",
    "max_decoder_seq_length = 162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "# All of the characters and substring that would mark lines in the training data as \"faulty\"\n",
    "invalid_chars = set(\n",
    "    [\n",
    "        \":\",\n",
    "        \"+\",\n",
    "        \"#\",\n",
    "        \"@\",\n",
    "        \"Ö\",\n",
    "        \"á\",\n",
    "        \"ä\",\n",
    "        \"é\",\n",
    "        \"í\",\n",
    "        \"ñ\",\n",
    "        \"ó\",\n",
    "        \"ö\",\n",
    "        \"ú\",\n",
    "        \"ā\",\n",
    "        \"Ć\",\n",
    "        \"ć\",\n",
    "        \"ʻ\",\n",
    "        \"́\",\n",
    "        \"е\",\n",
    "        \"н\",\n",
    "        \"о\",\n",
    "        \"п\",\n",
    "        \"у\",\n",
    "        \"ш\",\n",
    "        \"ã\",\n",
    "        \"ï\",\n",
    "        \"ō\",\n",
    "        \"ū\",\n",
    "        \"ί\",\n",
    "        \"α\",\n",
    "        \"δ\",\n",
    "        \"ε\",\n",
    "        \"κ\",\n",
    "        \"ο\",\n",
    "        \"в\",\n",
    "        \"ὐ\",\n",
    "        chr(776),\n",
    "        \"ç\",\n",
    "        \"ē\",\n",
    "        \"D\",\n",
    "        \"O\",\n",
    "        \"T\",\n",
    "    ]\n",
    ")\n",
    "invalid_chars_X = set([\"(\", \")\", \"<\", \">\", \"_\", \",\"])\n",
    "invalid_markers = set([\"\\\\F\", \"TrueP\", \"\\\\x\", \"semantics_error\", \"Prog(\"])\n",
    "files_with_compound_preds = [20, 21, 15]\n",
    "\n",
    "\n",
    "def mark_if_faulty(line, file_idx, X=False):\n",
    "    if X and (\n",
    "        any((c in invalid_chars) for c in line)\n",
    "        or any((c in invalid_chars_X) for c in line)\n",
    "    ):\n",
    "        return \"syntax_error\"\n",
    "    # TODO: Refactor this hacky workaround\n",
    "    if line[0] == \"(\" and file_idx not in files_with_compound_preds:\n",
    "        return \"syntax_error\"\n",
    "    if any((m in line) for m in invalid_markers) or any(\n",
    "        (c in invalid_chars) for c in line\n",
    "    ):\n",
    "        return \"syntax_error\"\n",
    "    # Remove top-level parentheses from lambda expression\n",
    "    if line[0:4] == \"(exi\" and line[-1] == \")\":\n",
    "        line = line[1:-1]\n",
    "    if line[0:4] == \"(all\" and line[-1] == \")\":\n",
    "        line = line[1:-1]\n",
    "\n",
    "    return line\n",
    "\n",
    "\n",
    "def lines_from_file(direc, name, drop_punc=False, lower=True, drop_fullstop=True):\n",
    "    with open(direc + name) as f:\n",
    "        for l in f:\n",
    "            l = l.rstrip()\n",
    "            if drop_punc:\n",
    "                l = l.translate(table)\n",
    "            if lower:\n",
    "                l = l.lower()\n",
    "            if drop_fullstop and not drop_punc:\n",
    "                l = l[0:-1]\n",
    "            yield l\n",
    "\n",
    "\n",
    "def load_and_clean_data(start_idx=1, end_idx=17, skip_idx_list=None):\n",
    "    X, y = [], []\n",
    "\n",
    "    err = lambda x: x == \"syntax_error\"\n",
    "    X_name = lambda i: f\"concordance_{i}_clean.txt\"\n",
    "    y_name = lambda i: f\"concordance_{i}_clean.lam\"\n",
    "\n",
    "    # Load lines from files and mark those that are \"faulty\"\n",
    "    for i in range(start_idx, end_idx + 1):\n",
    "        if i in skip_idx_list:\n",
    "            continue\n",
    "\n",
    "        X = X + [\n",
    "            mark_if_faulty(line, i, True)\n",
    "            for line in lines_from_file(X_train_dir, X_name(i), drop_fullstop=True)\n",
    "        ]\n",
    "        y = y + [\n",
    "            mark_if_faulty(line, i)\n",
    "            for line in lines_from_file(\n",
    "                y_train_dir, y_name(i), lower=False, drop_fullstop=False\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Save \"faulty\" line indices\n",
    "    err_idx_X = [i1 for i1 in range(len(X)) if err(X[i1])]\n",
    "    err_idx_y = [j1 for j1 in range(len(X)) if err(y[j1])]\n",
    "\n",
    "    err_idx = set(err_idx_X).union(set(err_idx_y))\n",
    "    num_err = len(err_idx)\n",
    "    num_samples = len(y) - num_err\n",
    "    clean_ratio = 100 - ((num_err / len(y)) * 100)\n",
    "\n",
    "    # Show stats about training data\n",
    "    print_stats(X, y, num_samples, num_err, clean_ratio)\n",
    "\n",
    "    # Remove \"faulty\" lines\n",
    "    for index in sorted(list(err_idx), reverse=True):\n",
    "        del X[index]\n",
    "        del y[index]\n",
    "\n",
    "    return (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats:\n",
      "------------------------\n",
      "------------------------\n",
      "N(X) == N(y) == 40000\n",
      "errs: 0\n",
      "Clean data (N = 40000) ratio: 100.0%\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    *load_and_clean_data(1, 20, [ ]), test_size=0.25, random_state=4, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 30000\n",
      "Number of unique input tokens: 35\n",
      "Number of unique output tokens: 43\n",
      "WARNING: NEW Max sequence length for inputs: 2465\n",
      "Dataset may be incompatible with older models.\n",
      "['\\t', '\\n', ' ', '&', '(', ')', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "for i in range(0, len(X_train)):\n",
    "    # SOS == '\\n'\n",
    "    # EOS == '\\t'\n",
    "    y_train[i] = \"\\t\" + y_train[i] + \"\\n\"\n",
    "\n",
    "    for char in X_train[i]:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in y_train[i]:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_X_len = max([len(txt) for txt in X_train])\n",
    "max_y_len = max([len(txt) for txt in y_train])\n",
    "\n",
    "print(\"Number of samples:\", len(X_train))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "\n",
    "if max_X_len > max_encoder_seq_length:\n",
    "    print(\"WARNING: NEW Max sequence length for inputs:\", max_X_len)\n",
    "    print(\"Dataset may be incompatible with older models.\")\n",
    "    max_encoder_seq_length = max_X_len\n",
    "\n",
    "if max_y_len > max_decoder_seq_length:\n",
    "    print(\"WARNING: NEW Max sequence length for outputs:\", max_y_len)\n",
    "    print(\"Dataset may be incompatible with older models.\")\n",
    "    max_decoder_seq_length = max_y_len\n",
    "\n",
    "print(target_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char to index\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(X_train), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(X_train), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(X_train), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "np.set_printoptions(threshold=10)\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(X_train, y_train)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
    "\n",
    "encoder_input_data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Test Data (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "\n",
    "# with open(X_test_dir + f\"{datetime.datetime()}_X_test.txt\", 'w+') as f:\n",
    "#     for line in X_test:\n",
    "#         f.write(f\"{line}\\n\")\n",
    "        \n",
    "# with open(X_test_dir + f\"{datetime.datetime()}_y_test.txt\", 'w+') as f:\n",
    "#     for line in y_test:\n",
    "#         f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(\n",
    "            name=\"W_a\",\n",
    "            shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.U_a = self.add_weight(\n",
    "            name=\"U_a\",\n",
    "            shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.V_a = self.add_weight(\n",
    "            name=\"V_a\",\n",
    "            shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        super(AttentionLayer, self).build(\n",
    "            input_shape\n",
    "        )  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print(\"encoder_out_seq>\", encoder_out_seq.shape)\n",
    "            print(\"decoder_out_seq>\", decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\"Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(\n",
    "                states, type(states)\n",
    "            )\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(\n",
    "                K.dot(inputs, self.U_a), 1\n",
    "            )  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print(\"Ua.h>\", U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "            if verbose:\n",
    "                print(\"Ws+Uh>\", Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"ei>\", e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(\n",
    "                states, type(states)\n",
    "            )\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print(\"ci>\", c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(\n",
    "            encoder_out_seq, axis=2\n",
    "        )  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step,\n",
    "            decoder_out_seq,\n",
    "            [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step,\n",
    "            e_outputs,\n",
    "            [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1])),\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    TimeDistributed,\n",
    "    Bidirectional,\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from livelossplot import PlotLossesKeras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla LSTM (Sutskever et al.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 300  # Number of epochs to train for.\n",
    "simple_name = \"lstm\"\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(\n",
    "    latent_dim, recurrent_dropout=0.1, return_state=True, name=\"encoder\"\n",
    ")\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(\n",
    "    latent_dim,\n",
    "    recurrent_dropout=0.1,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    name=\"decoder\",\n",
    ")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_pred = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "\n",
    "full_model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 96\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 300  # Number of epochs to train for.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = GRU(latent_dim, recurrent_dropout=0.333, return_state=True, name=\"encoder\")\n",
    "encoder_outputs, encoder_state = encoder(encoder_inputs)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_gru = GRU(\n",
    "    latent_dim,\n",
    "    recurrent_dropout=0.2,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    name=\"decoder\",\n",
    ")\n",
    "decoder_outputs, _ = decoder_gru(decoder_inputs, initial_state=encoder_state)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_pred = decoder_dense(decoder_outputs)\n",
    "\n",
    "early_stop = EarlyStopping(patience=3, monitor=\"val_loss\")\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "\n",
    "full_model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional GRU + Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 48\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 30  # Number of epochs to train for.\n",
    "recurrent_dropout_rate = 0.2\n",
    "dropout_rate = 0.5\n",
    "simple_name = \"bigru\"\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# Encoder GRU\n",
    "encoder_gru = Bidirectional(\n",
    "    GRU(\n",
    "        latent_dim,\n",
    "        return_sequences=True,\n",
    "        return_state=True,\n",
    "        name=\"encoder_gru\",\n",
    "        recurrent_dropout=recurrent_dropout_rate,\n",
    "    ),\n",
    "    name=\"bidirectional_encoder\",\n",
    ")\n",
    "encoder_out, encoder_fwd_state, encoder_back_state = encoder_gru(encoder_inputs)\n",
    "\n",
    "# Set up the decoder GRU, using `encoder_states` as initial state.\n",
    "decoder_gru = GRU(\n",
    "    latent_dim * 2,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    name=\"decoder_gru\",\n",
    "    recurrent_dropout=recurrent_dropout_rate,\n",
    ")\n",
    "decoder_out, decoder_state = decoder_gru(\n",
    "    decoder_inputs,\n",
    "    initial_state=Concatenate(axis=-1)([encoder_fwd_state, encoder_back_state]),\n",
    ")\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name=\"attention_layer\")\n",
    "attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "# Concat attention input and decoder GRU output\n",
    "decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")(\n",
    "    [decoder_out, attn_out]\n",
    ")\n",
    "\n",
    "# Dense layer\n",
    "dense = Dense(num_decoder_tokens, activation=\"softmax\", name=\"softmax_layer\")\n",
    "decoder_pred = dense(decoder_concat_input)\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "# opt = Adam(\n",
    "#     learning_rate=0.001,\n",
    "#     beta_1=0.9,\n",
    "#     beta_2=0.999,\n",
    "#     epsilon=1e-07,\n",
    "#     amsgrad=True,\n",
    "#     name='Adam'\n",
    "# )\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.0015,\n",
    "    rho=0.9,\n",
    "    momentum=0.0,\n",
    "    epsilon=1e-07,\n",
    "    centered=True,\n",
    "    name=\"RMSprop\",\n",
    ")\n",
    "\n",
    "# Full model\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "full_model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM + Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 44\n",
    "batch_size = 48  # Batch size for training.\n",
    "epochs = 30  # Number of epochs to train for.\n",
    "recurrent_dropout_rate = 0.2\n",
    "simple_name = \"bilstm\"\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# Encoder LSTM\n",
    "encoder_lstm = Bidirectional(\n",
    "    LSTM(\n",
    "        latent_dim,\n",
    "        return_sequences=True,\n",
    "        return_state=True,\n",
    "        name=\"encoder_lstm\",\n",
    "        recurrent_dropout=recurrent_dropout_rate,\n",
    "    ),\n",
    "    name=\"bidirectional_encoder\",\n",
    ")\n",
    "(\n",
    "    encoder_out,\n",
    "    encoder_fwd_state_h,\n",
    "    encoder_fwd_state_c,\n",
    "    encoder_back_state_h,\n",
    "    encoder_back_state_c,\n",
    ") = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(\n",
    "    latent_dim * 2,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_dropout=recurrent_dropout_rate,\n",
    "    name=\"decoder_lstm\",\n",
    ")\n",
    "decoder_out, _, _ = decoder_lstm(\n",
    "    decoder_inputs,\n",
    "    initial_state=[\n",
    "        Concatenate(axis=-1)([encoder_fwd_state_h, encoder_back_state_h]),\n",
    "        Concatenate(axis=-1)([encoder_fwd_state_c, encoder_back_state_c]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name=\"attention_layer\")\n",
    "attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")(\n",
    "    [decoder_out, attn_out]\n",
    ")\n",
    "\n",
    "# Dense layer\n",
    "dense = Dense(num_decoder_tokens, activation=\"softmax\", name=\"softmax_layer\")\n",
    "decoder_pred = dense(decoder_concat_input)\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "# opt = Adam(\n",
    "#     learning_rate=0.001,\n",
    "#     beta_1=0.9,\n",
    "#     beta_2=0.999,\n",
    "#     epsilon=1e-07,\n",
    "#     amsgrad=True,\n",
    "#     name=\"Adam\",\n",
    "# )\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.0015,\n",
    "    rho=0.9,\n",
    "    momentum=0.0,\n",
    "    epsilon=1e-07,\n",
    "    centered=True,\n",
    "    name=\"RMSprop\",\n",
    ")\n",
    "\n",
    "# Full model\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "full_model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_18 (InputLayer)          [(None, None, 35)]   0           []                               \n",
      "                                                                                                  \n",
      " bidirectional_encoder (Bidirec  [(None, None, 88),  28160       ['input_18[0][0]']               \n",
      " tional)                         (None, 44),                                                      \n",
      "                                 (None, 44),                                                      \n",
      "                                 (None, 44),                                                      \n",
      "                                 (None, 44)]                                                      \n",
      "                                                                                                  \n",
      " input_20 (InputLayer)          [(None, None, 43)]   0           []                               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 88)           0           ['bidirectional_encoder[0][1]',  \n",
      "                                                                  'bidirectional_encoder[0][3]']  \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 88)           0           ['bidirectional_encoder[0][2]',  \n",
      "                                                                  'bidirectional_encoder[0][4]']  \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 88),   46464       ['input_20[0][0]',               \n",
      "                                 (None, 88),                      'concatenate_4[0][0]',          \n",
      "                                 (None, 88)]                      'concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " attention_layer (AttentionLaye  ((None, None, 88),  15576       ['bidirectional_encoder[0][0]',  \n",
      " r)                              (None, None, None)               'decoder_lstm[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)     (None, None, 176)    0           ['decoder_lstm[0][0]',           \n",
      "                                                                  'attention_layer[0][0]']        \n",
      "                                                                                                  \n",
      " softmax_layer (Dense)          (None, None, 43)     7611        ['concat_layer[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 97,811\n",
      "Trainable params: 97,811\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (Run or Pass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "\n",
    "\n",
    "# log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# cp_path = f\"{config.CHECKPOINT_DIR}weights-{simple_name}-N({len(X_train)})-{latent_dim}.best.hdf5\"\n",
    "\n",
    "# # Load weights, if any from previous run\n",
    "# # full_model.load_weights(cp_path)\n",
    "\n",
    "# # Callbacks\n",
    "# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "# early_stop = EarlyStopping(patience=2, monitor=\"val_loss\", mode=\"min\", verbose=1)\n",
    "# plot_loss = PlotLossesKeras()\n",
    "# checkpoint = ModelCheckpoint(\n",
    "#     cp_path, save_weights_only=True, verbose=1, monitor=\"val_loss\", save_best_only=True\n",
    "# )\n",
    "\n",
    "# # Start Training\n",
    "# history = full_model.fit(\n",
    "#     [encoder_input_data, decoder_input_data],\n",
    "#     decoder_target_data,\n",
    "#     batch_size=batch_size,\n",
    "#     epochs=epochs,\n",
    "#     validation_split=0.1,\n",
    "#     shuffle=True,\n",
    "#     callbacks=[early_stop, plot_loss, checkpoint],\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(history.history.keys())\n",
    "# # summarize history for accuracy\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.plot(history.history['accuracy'])\n",
    "# plt.plot(history.history['val_accuracy'])\n",
    "# plt.title('model accuracy')\n",
    "# plt.ylabel('accuracy')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()\n",
    "# # summarize history for loss\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.title('model loss')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "# plt.legend(['train', 'test'], loc='upper left')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.save(f\"{config.MODELS_DIR}{simple_name}-N({len(X_train)})-{latent_dim}.best.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence_bigru_attn(\n",
    "    encoder_model, decoder_model, test_X_seq, num_encoder_tokens, num_decoder_tokens\n",
    "):\n",
    "    \"\"\"\n",
    "    Infer logic\n",
    "    :param encoder_model: keras.Model\n",
    "    :param decoder_model: keras.Model\n",
    "    :param test_X_seq: sequence of word ids\n",
    "    :param num_encoder_tokens: int\n",
    "    :param num_decoder_tokens: int\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    enc_outs, enc_fwd_state, enc_back_state = encoder_model.predict(test_X_seq)\n",
    "    dec_state = np.concatenate([enc_fwd_state, enc_back_state], axis=-1)\n",
    "\n",
    "    attention_weights = []\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        dec_out, attention, dec_state = decoder_model.predict(\n",
    "            [enc_outs, dec_state, target_seq]\n",
    "        )\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(dec_out, axis=-1)[0, 0]\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        attention_weights.append((sampled_token_index, attention))\n",
    "\n",
    "    return decoded_sentence, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence_bilstm_attn(\n",
    "    encoder_model, decoder_model, test_X_seq, num_encoder_tokens, num_decoder_tokens\n",
    "):\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    (\n",
    "        enc_outs,\n",
    "        enc_fwd_state_h,\n",
    "        enc_fwd_state_c,\n",
    "        enc_back_state_h,\n",
    "        enc_back_state_c,\n",
    "    ) = encoder_model.predict(test_X_seq)\n",
    "    encoder_state_h = np.concatenate([enc_fwd_state_h, enc_back_state_h], axis=-1)\n",
    "    encoder_state_c = np.concatenate([enc_fwd_state_c, enc_back_state_c], axis=-1)\n",
    "\n",
    "    # The ordering seems significant\n",
    "    # enc_outs, enc_fwd_state_h, enc_fwd_state_c, enc_back_state_h, enc_back_state_c = encoder_model.predict(test_X_seq)\n",
    "\n",
    "    attention_weights = []\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        dec_out, attention, dec_state_h, dec_state_c = decoder_model.predict(\n",
    "            [enc_outs, encoder_state_h, encoder_state_c, target_seq]\n",
    "        )\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(dec_out[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        attention_weights.append((sampled_token_index, attention))\n",
    "\n",
    "        # Update states\n",
    "        encoder_state_h = dec_state_h\n",
    "        encoder_state_c = dec_state_c\n",
    "\n",
    "    return decoded_sentence, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence_lstm(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(\n",
    "    encoder_inputs, attention_weights, en_id2word, fr_id2word, filename=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots attention weights\n",
    "    :param encoder_inputs: Sequence of word ids (list/numpy.ndarray)\n",
    "    :param attention_weights: Sequence of (<word_id_at_decode_step_t>:<attention_weights_at_decode_step_t>)\n",
    "    :param en_id2word: dict\n",
    "    :param fr_id2word: dict\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if len(attention_weights) == 0:\n",
    "        print(\n",
    "            \"Your attention weights were empty. No attention map saved to the disk. \"\n",
    "            + \"\\nPlease check if the decoder produced  a proper translation\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    mats = []\n",
    "    dec_inputs = []\n",
    "    for dec_ind, attn in attention_weights:\n",
    "        mats.append(attn.reshape(-1))\n",
    "        dec_inputs.append(dec_ind)\n",
    "    attention_mat = np.transpose(np.array(mats))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(32, 32))\n",
    "    ax.imshow(attention_mat)\n",
    "\n",
    "    ax.set_xticks(np.arange(attention_mat.shape[1]))\n",
    "    ax.set_yticks(np.arange(attention_mat.shape[0]))\n",
    "\n",
    "    ax.set_xticklabels([fr_id2word[inp] if inp != 0 else \"<Res>\" for inp in dec_inputs])\n",
    "    y_lab = [\n",
    "        en_id2word[inp] if inp != 0 else \"<Res>\"\n",
    "        for inp in [\n",
    "            np.argmax(np.squeeze(encoder_inputs)[i])\n",
    "            for i in range(0, np.squeeze(encoder_inputs).shape[0])\n",
    "        ]\n",
    "    ]\n",
    "    ax.set_yticklabels(y_lab)\n",
    "\n",
    "    ax.tick_params(labelsize=16)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "\n",
    "    if not os.path.exists(config.RESULTS_DIR):\n",
    "        os.mkdir(config.RESULTS_DIR)\n",
    "    if filename is None:\n",
    "        plt.savefig(os.path.join(config.RESULTS_DIR, \"attention.png\"))\n",
    "    else:\n",
    "        plt.savefig(os.path.join(config.RESULTS_DIR, \"{}\".format(filename)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_dim = 44\n",
    "\n",
    "# full_model.load_weights(\n",
    "#     f\"{config.CHECKPOINT_DIR}weights-{simple_name}-N({len(X_train)})-{latent_dim}.best.hdf5\"\n",
    "# )\n",
    "# loaded_model = full_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "# encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_model = Model(encoder_inputs, [state_h, state_c])\n",
    "\n",
    "# decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "# decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "#     decoder_inputs, initial_state=decoder_states_inputs\n",
    "# )\n",
    "# decoder_states = [state_h, state_c]\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# decoder_model = Model(\n",
    "#     [decoder_inputs] + decoder_states_inputs, \n",
    "#     [decoder_outputs] + decoder_states\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-GRU + Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" Encoder (Inference) model \"\"\"\n",
    "# encoder_inf_inputs = Input(shape=(None, num_encoder_tokens), name=\"encoder_inf_inputs\")\n",
    "# encoder_inf_out, encoder_inf_fwd_state, encoder_inf_back_state = encoder_gru(\n",
    "#     encoder_inf_inputs\n",
    "# )\n",
    "# encoder_model = Model(\n",
    "#     inputs=encoder_inf_inputs,\n",
    "#     outputs=[encoder_inf_out, encoder_inf_fwd_state, encoder_inf_back_state],\n",
    "# )\n",
    "\n",
    "# \"\"\" Decoder (Inference) model \"\"\"\n",
    "# decoder_inf_inputs = Input(shape=(None, num_decoder_tokens), name=\"decoder_inf_inputs\")\n",
    "# encoder_inf_states = Input(\n",
    "#     batch_shape=(None, None, 2 * latent_dim), name=\"encoder_inf_states\"\n",
    "# )\n",
    "# decoder_init_state = Input(batch_shape=(None, 2 * latent_dim), name=\"decoder_init\")\n",
    "\n",
    "# decoder_inf_out, decoder_inf_state = decoder_gru(\n",
    "#     decoder_inf_inputs, initial_state=decoder_init_state\n",
    "# )\n",
    "# attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "# decoder_inf_concat = Concatenate(axis=-1, name=\"concat\")(\n",
    "#     [decoder_inf_out, attn_inf_out]\n",
    "# )\n",
    "# decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "# decoder_model = Model(\n",
    "#     inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "#     outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM + Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Encoder (Inference) model \"\"\"\n",
    "encoder_inf_inputs = Input(shape=(None, num_encoder_tokens), name=\"encoder_inf_inputs\")\n",
    "(\n",
    "    encoder_inf_out,\n",
    "    encoder_inf_fwd_state_h,\n",
    "    encoder_inf_fwd_state_c,\n",
    "    encoder_inf_back_state_h,\n",
    "    encoder_inf_back_state_c,\n",
    ") = encoder_lstm(encoder_inf_inputs)\n",
    "encoder_model = Model(\n",
    "    inputs=encoder_inf_inputs,\n",
    "    outputs=[\n",
    "        encoder_inf_out,\n",
    "        encoder_inf_fwd_state_h,\n",
    "        encoder_inf_fwd_state_c,\n",
    "        encoder_inf_back_state_h,\n",
    "        encoder_inf_back_state_c,\n",
    "    ],\n",
    ")\n",
    "\n",
    "\"\"\" Decoder (Inference) model \"\"\"\n",
    "decoder_inf_inputs = Input(shape=(None, num_decoder_tokens), name=\"decoder_inf_inputs\")\n",
    "encoder_inf_states = Input(\n",
    "    batch_shape=(None, None, 2 * latent_dim), name=\"encoder_inf_states\"\n",
    ")\n",
    "\n",
    "decoder_state_input_h = Input(batch_shape=(None, 2 * latent_dim))\n",
    "decoder_state_input_c = Input(batch_shape=(None, 2 * latent_dim))\n",
    "\n",
    "decoder_init_state = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inf_out, decoder_inf_state_h, decoder_inf_state_c = decoder_lstm(\n",
    "    decoder_inf_inputs, initial_state=decoder_init_state\n",
    ")\n",
    "\n",
    "attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name=\"concat\")(\n",
    "    [decoder_inf_out, attn_inf_out]\n",
    ")\n",
    "decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "\n",
    "decoder_model = Model(\n",
    "    inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "    outputs=[\n",
    "        decoder_inf_pred,\n",
    "        attn_inf_states,\n",
    "        decoder_inf_state_h,\n",
    "        decoder_inf_state_c,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Saved HDF5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Set these parameters (embedded in file name)\n",
    "# latent_dim = 64\n",
    "\n",
    "# loaded_model = load_model(\n",
    "#     f\"{config.MODELS_DIR}{simple_name}-N({len(X_train)})-{latent_dim}.best.h5\",\n",
    "#     custom_objects={\"AttentionLayer\": AttentionLayer},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(loaded_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "# encoder = loaded_model.layers[2]\n",
    "# encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# encoder_model = Model(encoder_inputs, [state_h, state_c])\n",
    "\n",
    "# decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# decoder_dense = loaded_model.layers[4]\n",
    "\n",
    "# decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "# decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# decoder_lstm = loaded_model.layers[3]\n",
    "# decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "#     decoder_inputs, initial_state=decoder_states_inputs\n",
    "# )\n",
    "# decoder_states = [state_h, state_c]\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# decoder_model = Model(\n",
    "#     [decoder_inputs] + decoder_states_inputs, \n",
    "#     [decoder_outputs] + decoder_states\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-GRU + Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inf_inputs = Input(shape=(None, num_encoder_tokens), name=\"encoder_inf_inputs\")\n",
    "# encoder = full_model.layers[1]\n",
    "# encoder_inf_out, encoder_inf_fwd_state, encoder_inf_back_state = encoder(\n",
    "#     encoder_inf_inputs\n",
    "# )\n",
    "# encoder_model = Model(\n",
    "#     inputs=encoder_inf_inputs,\n",
    "#     outputs=[encoder_inf_out, encoder_inf_fwd_state, encoder_inf_back_state],\n",
    "# )\n",
    "\n",
    "# \"\"\" Decoder (Inference) model \"\"\"\n",
    "# decoder_inf_inputs = Input(shape=(None, num_decoder_tokens), name=\"decoder_inf_inputs\")\n",
    "# encoder_inf_states = Input(\n",
    "#     batch_shape=(None, None, 2 * latent_dim), name=\"encoder_inf_states\"\n",
    "# )\n",
    "# decoder_init_state = Input(batch_shape=(None, 2 * latent_dim), name=\"decoder_init\")\n",
    "\n",
    "# decoder = full_model.layers[4]\n",
    "# dense = full_model.layers[7]\n",
    "# decoder_inf_out, decoder_inf_state = decoder(\n",
    "#     decoder_inf_inputs, initial_state=decoder_init_state\n",
    "# )\n",
    "# attn_layer = full_model.layers[5]\n",
    "# attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "# decoder_inf_concat = Concatenate(axis=-1, name=\"concat\")(\n",
    "#     [decoder_inf_out, attn_inf_out]\n",
    "# )\n",
    "# decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "# decoder_model = Model(\n",
    "#     inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "#     outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 142ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 9ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "-\n",
      "Input sentence: info dfs fsnamesystem block\n",
      "Decoded sentence: '.999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999'\n",
      "Real sentence: 'exists x1.(_addstoredblock(x1) & exists x2.(_blk(x2) & (x1 = x2)))'\n",
      "INCORRECT\n",
      "Completed: 100.0%\n",
      "1/1\n",
      "Accuracy:\n",
      "0.0%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3200x3200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test = [\"info dfs fsnamesystem block namesystem addstoredblock blockmap updated 10 251 43 115 50010 is added to blk 3050920587428079149 size 67108864\"]\n",
    "\n",
    "encoder_test_data = np.zeros(\n",
    "    (len(X_test), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "for i, x in enumerate(X_test):\n",
    "    for t, char in enumerate(x):\n",
    "        encoder_test_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_test_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "\n",
    "import sys\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "correct = 0\n",
    "checked = 0\n",
    "for seq_index in range(0, len(X_test)):\n",
    "    test_X = X_test[seq_index]\n",
    "    input_seq = encoder_test_data[seq_index : seq_index + 1]\n",
    "\n",
    "    # Bi-LSTM\n",
    "    decoded_sentence, attn_weights = decode_sequence_bilstm_attn(\n",
    "        encoder_model, decoder_model, input_seq, num_encoder_tokens, num_decoder_tokens\n",
    "    )\n",
    "\n",
    "    # LSTM\n",
    "    # decoded_sentence = decode_sequence_lstm(input_seq)\n",
    "    plot_attention_weights(\n",
    "        input_seq,\n",
    "        attn_weights,\n",
    "        reverse_input_char_index,\n",
    "        reverse_target_char_index,\n",
    "        filename=\"attention_{}.png\".format(seq_index),\n",
    "    )\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", X_test[seq_index])\n",
    "    print(\"Decoded sentence:\", repr(decoded_sentence.rstrip()))\n",
    "    print(\"Real sentence:\", repr(y_test[seq_index]))\n",
    "    print(\"CORRECT\" if decoded_sentence.rstrip() == y_test[seq_index] else \"INCORRECT\")\n",
    "    correct += 1 if decoded_sentence.rstrip() == y_test[seq_index] else 0\n",
    "    checked += 1\n",
    "    print(f\"Completed: {(checked / len(X_test)) * 100}%\")\n",
    "    print(f\"{checked}/{len(X_test)}\")\n",
    "    print(\"Accuracy:\")\n",
    "    print(f\"{(correct / checked) * 100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "66d4fb87ddb4494270580ee563fb6840ad68ac8d4139104e541e8f4c217ec4ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
