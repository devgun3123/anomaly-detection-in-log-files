{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.15\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    PROJECT_DIR = os.environ[\"PWD\"]\n",
    "    DATA_DIR = os.getenv(\"DATA_DIR\", \"dat/\")\n",
    "    RESULTS_DIR = os.getenv(\"RESULTS_DIR\", \"results/\")\n",
    "    MODELS_DIR = os.getenv(\"MODELS_DIR\", \"models/\")\n",
    "    CHECKPOINT_DIR = os.getenv(\"CHECKPOINT_DIR\", \"models/checkpoint/\")\n",
    "    LOGS_DIR = os.getenv(\"LOGS_DIR\", \"logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(X, y, num_s, num_e, ratio):\n",
    "    print('Stats:')\n",
    "    print(\"------------------------\")\n",
    "    print(\"------------------------\")\n",
    "    print(f'N(X) == N(y) == {len(y)}')\n",
    "    print(f'errs: {num_e}')\n",
    "    print(f'Clean data (N = {num_s}) ratio: {ratio}%')\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "X_train_dir = f\"{config.DATA_DIR}clean/\"\n",
    "y_train_dir = f\"{config.DATA_DIR}trans/\"\n",
    "X_test_dir = f\"{config.DATA_DIR}test/\"\n",
    "\n",
    "# Will notify if these values change\n",
    "max_encoder_seq_length = 81\n",
    "max_decoder_seq_length = 162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "# All of the characters and substring that would mark lines in the training data as \"faulty\"\n",
    "invalid_chars = set(\n",
    "    [\n",
    "        \":\",\n",
    "        \"+\",\n",
    "        \"#\",\n",
    "        \"@\",\n",
    "        \"Ö\",\n",
    "        \"á\",\n",
    "        \"ä\",\n",
    "        \"é\",\n",
    "        \"í\",\n",
    "        \"ñ\",\n",
    "        \"ó\",\n",
    "        \"ö\",\n",
    "        \"ú\",\n",
    "        \"ā\",\n",
    "        \"Ć\",\n",
    "        \"ć\",\n",
    "        \"ʻ\",\n",
    "        \"́\",\n",
    "        \"е\",\n",
    "        \"н\",\n",
    "        \"о\",\n",
    "        \"п\",\n",
    "        \"у\",\n",
    "        \"ш\",\n",
    "        \"ã\",\n",
    "        \"ï\",\n",
    "        \"ō\",\n",
    "        \"ū\",\n",
    "        \"ί\",\n",
    "        \"α\",\n",
    "        \"δ\",\n",
    "        \"ε\",\n",
    "        \"κ\",\n",
    "        \"ο\",\n",
    "        \"в\",\n",
    "        \"ὐ\",\n",
    "        chr(776),\n",
    "        \"ç\",\n",
    "        \"ē\",\n",
    "        \"D\",\n",
    "        \"O\",\n",
    "        \"T\",\n",
    "    ]\n",
    ")\n",
    "invalid_chars_X = set([\"(\", \")\", \"<\", \">\", \"_\", \",\"])\n",
    "invalid_markers = set([\"\\\\F\", \"TrueP\", \"\\\\x\", \"semantics_error\", \"Prog(\"])\n",
    "files_with_compound_preds = [20, 21, 15]\n",
    "\n",
    "\n",
    "def mark_if_faulty(line, file_idx, X=False):\n",
    "    if X and (\n",
    "        any((c in invalid_chars) for c in line)\n",
    "        or any((c in invalid_chars_X) for c in line)\n",
    "    ):\n",
    "        return \"syntax_error\"\n",
    "    # TODO: Refactor this hacky workaround\n",
    "    if line[0] == \"(\" and file_idx not in files_with_compound_preds:\n",
    "        return \"syntax_error\"\n",
    "    if any((m in line) for m in invalid_markers) or any(\n",
    "        (c in invalid_chars) for c in line\n",
    "    ):\n",
    "        return \"syntax_error\"\n",
    "    # Remove top-level parentheses from lambda expression\n",
    "    if line[0:4] == \"(exi\" and line[-1] == \")\":\n",
    "        line = line[1:-1]\n",
    "    if line[0:4] == \"(all\" and line[-1] == \")\":\n",
    "        line = line[1:-1]\n",
    "\n",
    "    return line\n",
    "\n",
    "\n",
    "def lines_from_file(direc, name, drop_punc=False, lower=True, drop_fullstop=True):\n",
    "    with open(direc + name) as f:\n",
    "        for l in f:\n",
    "            l = l.rstrip()\n",
    "            if drop_punc:\n",
    "                l = l.translate(table)\n",
    "            if lower:\n",
    "                l = l.lower()\n",
    "            if drop_fullstop and not drop_punc:\n",
    "                l = l[0:-1]\n",
    "            yield l\n",
    "\n",
    "\n",
    "def load_and_clean_data(start_idx=1, end_idx=17, skip_idx_list=None):\n",
    "    X, y = [], []\n",
    "\n",
    "    err = lambda x: x == \"syntax_error\"\n",
    "    X_name = lambda i: f\"concordance_{i}_clean.txt\"\n",
    "    y_name = lambda i: f\"concordance_{i}_clean.lam\"\n",
    "\n",
    "    # Load lines from files and mark those that are \"faulty\"\n",
    "    for i in range(start_idx, end_idx + 1):\n",
    "        if i in skip_idx_list:\n",
    "            continue\n",
    "\n",
    "        X = X + [\n",
    "            mark_if_faulty(line, i, True)\n",
    "            for line in lines_from_file(X_train_dir, X_name(i), drop_fullstop=True)\n",
    "        ]\n",
    "        y = y + [\n",
    "            mark_if_faulty(line, i)\n",
    "            for line in lines_from_file(\n",
    "                y_train_dir, y_name(i), lower=False, drop_fullstop=False\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Save \"faulty\" line indices\n",
    "    err_idx_X = [i1 for i1 in range(len(X)) if err(X[i1])]\n",
    "    err_idx_y = [j1 for j1 in range(len(X)) if err(y[j1])]\n",
    "\n",
    "    err_idx = set(err_idx_X).union(set(err_idx_y))\n",
    "    num_err = len(err_idx)\n",
    "    num_samples = len(y) - num_err\n",
    "    clean_ratio = 100 - ((num_err / len(y)) * 100)\n",
    "\n",
    "    # Show stats about training data\n",
    "    print_stats(X, y, num_samples, num_err, clean_ratio)\n",
    "\n",
    "    # Remove \"faulty\" lines\n",
    "    for index in sorted(list(err_idx), reverse=True):\n",
    "        del X[index]\n",
    "        del y[index]\n",
    "\n",
    "    return (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats:\n",
      "------------------------\n",
      "------------------------\n",
      "N(X) == N(y) == 40000\n",
      "errs: 0\n",
      "Clean data (N = 40000) ratio: 100.0%\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    *load_and_clean_data(1, 20, [ ]), test_size=0.25, random_state=4, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 30000\n",
      "Number of unique input tokens: 35\n",
      "Number of unique output tokens: 43\n",
      "WARNING: NEW Max sequence length for inputs: 2465\n",
      "Dataset may be incompatible with older models.\n",
      "['\\t', '\\n', ' ', '&', '(', ')', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "for i in range(0, len(X_train)):\n",
    "    # SOS == '\\n'\n",
    "    # EOS == '\\t'\n",
    "    y_train[i] = \"\\t\" + y_train[i] + \"\\n\"\n",
    "\n",
    "    for char in X_train[i]:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in y_train[i]:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_X_len = max([len(txt) for txt in X_train])\n",
    "max_y_len = max([len(txt) for txt in y_train])\n",
    "\n",
    "print(\"Number of samples:\", len(X_train))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "\n",
    "if max_X_len > max_encoder_seq_length:\n",
    "    print(\"WARNING: NEW Max sequence length for inputs:\", max_X_len)\n",
    "    print(\"Dataset may be incompatible with older models.\")\n",
    "    max_encoder_seq_length = max_X_len\n",
    "\n",
    "if max_y_len > max_decoder_seq_length:\n",
    "    print(\"WARNING: NEW Max sequence length for outputs:\", max_y_len)\n",
    "    print(\"Dataset may be incompatible with older models.\")\n",
    "    max_decoder_seq_length = max_y_len\n",
    "\n",
    "print(target_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char to index\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(X_train), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(X_train), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(X_train), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "np.set_printoptions(threshold=10)\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(X_train, y_train)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
    "\n",
    "encoder_input_data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Test Data (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "\n",
    "# with open(X_test_dir + f\"{datetime.datetime()}_X_test.txt\", 'w+') as f:\n",
    "#     for line in X_test:\n",
    "#         f.write(f\"{line}\\n\")\n",
    "        \n",
    "# with open(X_test_dir + f\"{datetime.datetime()}_y_test.txt\", 'w+') as f:\n",
    "#     for line in y_test:\n",
    "#         f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(\n",
    "            name=\"W_a\",\n",
    "            shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.U_a = self.add_weight(\n",
    "            name=\"U_a\",\n",
    "            shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.V_a = self.add_weight(\n",
    "            name=\"V_a\",\n",
    "            shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        super(AttentionLayer, self).build(\n",
    "            input_shape\n",
    "        )  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print(\"encoder_out_seq>\", encoder_out_seq.shape)\n",
    "            print(\"decoder_out_seq>\", decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\"Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(\n",
    "                states, type(states)\n",
    "            )\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(\n",
    "                K.dot(inputs, self.U_a), 1\n",
    "            )  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print(\"Ua.h>\", U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "            if verbose:\n",
    "                print(\"Ws+Uh>\", Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"ei>\", e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(\n",
    "                states, type(states)\n",
    "            )\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print(\"ci>\", c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(\n",
    "            encoder_out_seq, axis=2\n",
    "        )  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step,\n",
    "            decoder_out_seq,\n",
    "            [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step,\n",
    "            e_outputs,\n",
    "            [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1])),\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    TimeDistributed,\n",
    "    Bidirectional,\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from livelossplot import PlotLossesKeras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla LSTM (Sutskever et al.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 300  # Number of epochs to train for.\n",
    "simple_name = \"lstm\"\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(\n",
    "    latent_dim, recurrent_dropout=0.1, return_state=True, name=\"encoder\"\n",
    ")\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(\n",
    "    latent_dim,\n",
    "    recurrent_dropout=0.1,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    name=\"decoder\",\n",
    ")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_pred = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "\n",
    "full_model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 96\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 300  # Number of epochs to train for.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = GRU(latent_dim, recurrent_dropout=0.333, return_state=True, name=\"encoder\")\n",
    "encoder_outputs, encoder_state = encoder(encoder_inputs)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_gru = GRU(\n",
    "    latent_dim,\n",
    "    recurrent_dropout=0.2,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    name=\"decoder\",\n",
    ")\n",
    "decoder_outputs, _ = decoder_gru(decoder_inputs, initial_state=encoder_state)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_pred = decoder_dense(decoder_outputs)\n",
    "\n",
    "early_stop = EarlyStopping(patience=3, monitor=\"val_loss\")\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "\n",
    "full_model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional GRU + Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 48\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 30  # Number of epochs to train for.\n",
    "recurrent_dropout_rate = 0.2\n",
    "dropout_rate = 0.5\n",
    "simple_name = \"bigru\"\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# Encoder GRU\n",
    "encoder_gru = Bidirectional(\n",
    "    GRU(\n",
    "        latent_dim,\n",
    "        return_sequences=True,\n",
    "        return_state=True,\n",
    "        name=\"encoder_gru\",\n",
    "        recurrent_dropout=recurrent_dropout_rate,\n",
    "    ),\n",
    "    name=\"bidirectional_encoder\",\n",
    ")\n",
    "encoder_out, encoder_fwd_state, encoder_back_state = encoder_gru(encoder_inputs)\n",
    "\n",
    "# Set up the decoder GRU, using `encoder_states` as initial state.\n",
    "decoder_gru = GRU(\n",
    "    latent_dim * 2,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    name=\"decoder_gru\",\n",
    "    recurrent_dropout=recurrent_dropout_rate,\n",
    ")\n",
    "decoder_out, decoder_state = decoder_gru(\n",
    "    decoder_inputs,\n",
    "    initial_state=Concatenate(axis=-1)([encoder_fwd_state, encoder_back_state]),\n",
    ")\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name=\"attention_layer\")\n",
    "attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "# Concat attention input and decoder GRU output\n",
    "decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")(\n",
    "    [decoder_out, attn_out]\n",
    ")\n",
    "\n",
    "# Dense layer\n",
    "dense = Dense(num_decoder_tokens, activation=\"softmax\", name=\"softmax_layer\")\n",
    "decoder_pred = dense(decoder_concat_input)\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "# opt = Adam(\n",
    "#     learning_rate=0.001,\n",
    "#     beta_1=0.9,\n",
    "#     beta_2=0.999,\n",
    "#     epsilon=1e-07,\n",
    "#     amsgrad=True,\n",
    "#     name='Adam'\n",
    "# )\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.0015,\n",
    "    rho=0.9,\n",
    "    momentum=0.0,\n",
    "    epsilon=1e-07,\n",
    "    centered=True,\n",
    "    name=\"RMSprop\",\n",
    ")\n",
    "\n",
    "# Full model\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "full_model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM + Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 44\n",
    "batch_size = 48  # Batch size for training.\n",
    "epochs = 30  # Number of epochs to train for.\n",
    "recurrent_dropout_rate = 0.2\n",
    "simple_name = \"bilstm\"\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# Encoder LSTM\n",
    "encoder_lstm = Bidirectional(\n",
    "    LSTM(\n",
    "        latent_dim,\n",
    "        return_sequences=True,\n",
    "        return_state=True,\n",
    "        name=\"encoder_lstm\",\n",
    "        recurrent_dropout=recurrent_dropout_rate,\n",
    "    ),\n",
    "    name=\"bidirectional_encoder\",\n",
    ")\n",
    "(\n",
    "    encoder_out,\n",
    "    encoder_fwd_state_h,\n",
    "    encoder_fwd_state_c,\n",
    "    encoder_back_state_h,\n",
    "    encoder_back_state_c,\n",
    ") = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(\n",
    "    latent_dim * 2,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_dropout=recurrent_dropout_rate,\n",
    "    name=\"decoder_lstm\",\n",
    ")\n",
    "decoder_out, _, _ = decoder_lstm(\n",
    "    decoder_inputs,\n",
    "    initial_state=[\n",
    "        Concatenate(axis=-1)([encoder_fwd_state_h, encoder_back_state_h]),\n",
    "        Concatenate(axis=-1)([encoder_fwd_state_c, encoder_back_state_c]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name=\"attention_layer\")\n",
    "attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")(\n",
    "    [decoder_out, attn_out]\n",
    ")\n",
    "\n",
    "# Dense layer\n",
    "dense = Dense(num_decoder_tokens, activation=\"softmax\", name=\"softmax_layer\")\n",
    "decoder_pred = dense(decoder_concat_input)\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "# opt = Adam(\n",
    "#     learning_rate=0.001,\n",
    "#     beta_1=0.9,\n",
    "#     beta_2=0.999,\n",
    "#     epsilon=1e-07,\n",
    "#     amsgrad=True,\n",
    "#     name=\"Adam\",\n",
    "# )\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.0015,\n",
    "    rho=0.9,\n",
    "    momentum=0.0,\n",
    "    epsilon=1e-07,\n",
    "    centered=True,\n",
    "    name=\"RMSprop\",\n",
    ")\n",
    "\n",
    "# Full model\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "full_model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_16 (InputLayer)          [(None, None, 35)]   0           []                               \n",
      "                                                                                                  \n",
      " bidirectional_encoder (Bidirec  [(None, None, 88),  28160       ['input_16[0][0]']               \n",
      " tional)                         (None, 44),                                                      \n",
      "                                 (None, 44),                                                      \n",
      "                                 (None, 44),                                                      \n",
      "                                 (None, 44)]                                                      \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, None, 43)]   0           []                               \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 88)           0           ['bidirectional_encoder[0][1]',  \n",
      "                                                                  'bidirectional_encoder[0][3]']  \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 88)           0           ['bidirectional_encoder[0][2]',  \n",
      "                                                                  'bidirectional_encoder[0][4]']  \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 88),   46464       ['input_18[0][0]',               \n",
      "                                 (None, 88),                      'concatenate_4[0][0]',          \n",
      "                                 (None, 88)]                      'concatenate_5[0][0]']          \n",
      "                                                                                                  \n",
      " attention_layer (AttentionLaye  ((None, None, 88),  15576       ['bidirectional_encoder[0][0]',  \n",
      " r)                              (None, None, None)               'decoder_lstm[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)     (None, None, 176)    0           ['decoder_lstm[0][0]',           \n",
      "                                                                  'attention_layer[0][0]']        \n",
      "                                                                                                  \n",
      " softmax_layer (Dense)          (None, None, 43)     7611        ['concat_layer[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 97,811\n",
      "Trainable params: 97,811\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training (Run or Pass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "\ttraining         \t (min:    0.907, max:    0.907, cur:    0.907)\n",
      "\tvalidation       \t (min:    0.976, max:    0.976, cur:    0.976)\n",
      "Loss\n",
      "\ttraining         \t (min:    0.361, max:    0.361, cur:    0.361)\n",
      "\tvalidation       \t (min:    0.078, max:    0.078, cur:    0.078)\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.07823, saving model to models/checkpoint/weights-bilstm-N(30000)-44.best.hdf5\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to create file (unable to open file: name = 'models/checkpoint/weights-bilstm-N(30000)-44.best.hdf5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [40], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m     15\u001b[0m     cp_path, save_weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[39m# Start Training\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m history \u001b[39m=\u001b[39m full_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     20\u001b[0m     [encoder_input_data, decoder_input_data],\n\u001b[1;32m     21\u001b[0m     decoder_target_data,\n\u001b[1;32m     22\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     23\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     24\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     25\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     26\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stop, plot_loss, checkpoint],\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/log-nlp/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/anaconda3/envs/log-nlp/lib/python3.8/site-packages/h5py/_hl/files.py:533\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, **kwds)\u001b[0m\n\u001b[1;32m    525\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    526\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    527\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[1;32m    528\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[1;32m    529\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    530\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[1;32m    531\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[1;32m    532\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 533\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39mswmr)\n\u001b[1;32m    535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    536\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[0;32m/opt/anaconda3/envs/log-nlp/lib/python3.8/site-packages/h5py/_hl/files.py:232\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    230\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mcreate(name, h5f\u001b[39m.\u001b[39mACC_EXCL, fapl\u001b[39m=\u001b[39mfapl, fcpl\u001b[39m=\u001b[39mfcpl)\n\u001b[1;32m    231\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 232\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mcreate(name, h5f\u001b[39m.\u001b[39;49mACC_TRUNC, fapl\u001b[39m=\u001b[39;49mfapl, fcpl\u001b[39m=\u001b[39;49mfcpl)\n\u001b[1;32m    233\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    234\u001b[0m     \u001b[39m# Open in append mode (read/write).\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[39m# If that fails, create a new file only if it won't clobber an\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[39m# existing one (ACC_EXCL)\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:126\u001b[0m, in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to create file (unable to open file: name = 'models/checkpoint/weights-bilstm-N(30000)-44.best.hdf5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "cp_path = f\"{config.CHECKPOINT_DIR}weights-{simple_name}-N({len(X_train)})-{latent_dim}.best.hdf5\"\n",
    "\n",
    "# Load weights, if any from previous run\n",
    "# full_model.load_weights(cp_path)\n",
    "\n",
    "# Callbacks\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stop = EarlyStopping(patience=2, monitor=\"val_loss\", mode=\"min\", verbose=1)\n",
    "plot_loss = PlotLossesKeras()\n",
    "checkpoint = ModelCheckpoint(\n",
    "    cp_path, save_weights_only=True, verbose=1, monitor=\"val_loss\", save_best_only=True\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "history = full_model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stop, plot_loss, checkpoint],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.save(f\"{config.MODELS_DIR}{simple_name}-N({len(X_train)})-{latent_dim}.best.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence_bigru_attn(\n",
    "    encoder_model, decoder_model, test_X_seq, num_encoder_tokens, num_decoder_tokens\n",
    "):\n",
    "    \"\"\"\n",
    "    Infer logic\n",
    "    :param encoder_model: keras.Model\n",
    "    :param decoder_model: keras.Model\n",
    "    :param test_X_seq: sequence of word ids\n",
    "    :param num_encoder_tokens: int\n",
    "    :param num_decoder_tokens: int\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    enc_outs, enc_fwd_state, enc_back_state = encoder_model.predict(test_X_seq)\n",
    "    dec_state = np.concatenate([enc_fwd_state, enc_back_state], axis=-1)\n",
    "\n",
    "    attention_weights = []\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        dec_out, attention, dec_state = decoder_model.predict(\n",
    "            [enc_outs, dec_state, target_seq]\n",
    "        )\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(dec_out, axis=-1)[0, 0]\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        attention_weights.append((sampled_token_index, attention))\n",
    "\n",
    "    return decoded_sentence, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence_bilstm_attn(\n",
    "    encoder_model, decoder_model, test_X_seq, num_encoder_tokens, num_decoder_tokens\n",
    "):\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    (\n",
    "        enc_outs,\n",
    "        enc_fwd_state_h,\n",
    "        enc_fwd_state_c,\n",
    "        enc_back_state_h,\n",
    "        enc_back_state_c,\n",
    "    ) = encoder_model.predict(test_X_seq)\n",
    "    encoder_state_h = np.concatenate([enc_fwd_state_h, enc_back_state_h], axis=-1)\n",
    "    encoder_state_c = np.concatenate([enc_fwd_state_c, enc_back_state_c], axis=-1)\n",
    "\n",
    "    # The ordering seems significant\n",
    "    # enc_outs, enc_fwd_state_h, enc_fwd_state_c, enc_back_state_h, enc_back_state_c = encoder_model.predict(test_X_seq)\n",
    "\n",
    "    attention_weights = []\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        dec_out, attention, dec_state_h, dec_state_c = decoder_model.predict(\n",
    "            [enc_outs, encoder_state_h, encoder_state_c, target_seq]\n",
    "        )\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(dec_out[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        attention_weights.append((sampled_token_index, attention))\n",
    "\n",
    "        # Update states\n",
    "        encoder_state_h = dec_state_h\n",
    "        encoder_state_c = dec_state_c\n",
    "\n",
    "    return decoded_sentence, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence_lstm(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(\n",
    "    encoder_inputs, attention_weights, en_id2word, fr_id2word, filename=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots attention weights\n",
    "    :param encoder_inputs: Sequence of word ids (list/numpy.ndarray)\n",
    "    :param attention_weights: Sequence of (<word_id_at_decode_step_t>:<attention_weights_at_decode_step_t>)\n",
    "    :param en_id2word: dict\n",
    "    :param fr_id2word: dict\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if len(attention_weights) == 0:\n",
    "        print(\n",
    "            \"Your attention weights were empty. No attention map saved to the disk. \"\n",
    "            + \"\\nPlease check if the decoder produced  a proper translation\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    mats = []\n",
    "    dec_inputs = []\n",
    "    for dec_ind, attn in attention_weights:\n",
    "        mats.append(attn.reshape(-1))\n",
    "        dec_inputs.append(dec_ind)\n",
    "    attention_mat = np.transpose(np.array(mats))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(32, 32))\n",
    "    ax.imshow(attention_mat)\n",
    "\n",
    "    ax.set_xticks(np.arange(attention_mat.shape[1]))\n",
    "    ax.set_yticks(np.arange(attention_mat.shape[0]))\n",
    "\n",
    "    ax.set_xticklabels([fr_id2word[inp] if inp != 0 else \"<Res>\" for inp in dec_inputs])\n",
    "    y_lab = [\n",
    "        en_id2word[inp] if inp != 0 else \"<Res>\"\n",
    "        for inp in [\n",
    "            np.argmax(np.squeeze(encoder_inputs)[i])\n",
    "            for i in range(0, np.squeeze(encoder_inputs).shape[0])\n",
    "        ]\n",
    "    ]\n",
    "    ax.set_yticklabels(y_lab)\n",
    "\n",
    "    ax.tick_params(labelsize=16)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "\n",
    "    if not os.path.exists(config.RESULTS_DIR):\n",
    "        os.mkdir(config.RESULTS_DIR)\n",
    "    if filename is None:\n",
    "        plt.savefig(os.path.join(config.RESULTS_DIR, \"attention.png\"))\n",
    "    else:\n",
    "        plt.savefig(os.path.join(config.RESULTS_DIR, \"{}\".format(filename)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_dim = 44\n",
    "\n",
    "# full_model.load_weights(\n",
    "#     f\"{config.CHECKPOINT_DIR}weights-{simple_name}-N({len(X_train)})-{latent_dim}.best.hdf5\"\n",
    "# )\n",
    "# loaded_model = full_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "# encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_model = Model(encoder_inputs, [state_h, state_c])\n",
    "\n",
    "# decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "# decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "#     decoder_inputs, initial_state=decoder_states_inputs\n",
    "# )\n",
    "# decoder_states = [state_h, state_c]\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# decoder_model = Model(\n",
    "#     [decoder_inputs] + decoder_states_inputs, \n",
    "#     [decoder_outputs] + decoder_states\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-GRU + Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" Encoder (Inference) model \"\"\"\n",
    "# encoder_inf_inputs = Input(shape=(None, num_encoder_tokens), name=\"encoder_inf_inputs\")\n",
    "# encoder_inf_out, encoder_inf_fwd_state, encoder_inf_back_state = encoder_gru(\n",
    "#     encoder_inf_inputs\n",
    "# )\n",
    "# encoder_model = Model(\n",
    "#     inputs=encoder_inf_inputs,\n",
    "#     outputs=[encoder_inf_out, encoder_inf_fwd_state, encoder_inf_back_state],\n",
    "# )\n",
    "\n",
    "# \"\"\" Decoder (Inference) model \"\"\"\n",
    "# decoder_inf_inputs = Input(shape=(None, num_decoder_tokens), name=\"decoder_inf_inputs\")\n",
    "# encoder_inf_states = Input(\n",
    "#     batch_shape=(None, None, 2 * latent_dim), name=\"encoder_inf_states\"\n",
    "# )\n",
    "# decoder_init_state = Input(batch_shape=(None, 2 * latent_dim), name=\"decoder_init\")\n",
    "\n",
    "# decoder_inf_out, decoder_inf_state = decoder_gru(\n",
    "#     decoder_inf_inputs, initial_state=decoder_init_state\n",
    "# )\n",
    "# attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "# decoder_inf_concat = Concatenate(axis=-1, name=\"concat\")(\n",
    "#     [decoder_inf_out, attn_inf_out]\n",
    "# )\n",
    "# decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "# decoder_model = Model(\n",
    "#     inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "#     outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM + Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Encoder (Inference) model \"\"\"\n",
    "encoder_inf_inputs = Input(shape=(None, num_encoder_tokens), name=\"encoder_inf_inputs\")\n",
    "(\n",
    "    encoder_inf_out,\n",
    "    encoder_inf_fwd_state_h,\n",
    "    encoder_inf_fwd_state_c,\n",
    "    encoder_inf_back_state_h,\n",
    "    encoder_inf_back_state_c,\n",
    ") = encoder_lstm(encoder_inf_inputs)\n",
    "encoder_model = Model(\n",
    "    inputs=encoder_inf_inputs,\n",
    "    outputs=[\n",
    "        encoder_inf_out,\n",
    "        encoder_inf_fwd_state_h,\n",
    "        encoder_inf_fwd_state_c,\n",
    "        encoder_inf_back_state_h,\n",
    "        encoder_inf_back_state_c,\n",
    "    ],\n",
    ")\n",
    "\n",
    "\"\"\" Decoder (Inference) model \"\"\"\n",
    "decoder_inf_inputs = Input(shape=(None, num_decoder_tokens), name=\"decoder_inf_inputs\")\n",
    "encoder_inf_states = Input(\n",
    "    batch_shape=(None, None, 2 * latent_dim), name=\"encoder_inf_states\"\n",
    ")\n",
    "\n",
    "decoder_state_input_h = Input(batch_shape=(None, 2 * latent_dim))\n",
    "decoder_state_input_c = Input(batch_shape=(None, 2 * latent_dim))\n",
    "\n",
    "decoder_init_state = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inf_out, decoder_inf_state_h, decoder_inf_state_c = decoder_lstm(\n",
    "    decoder_inf_inputs, initial_state=decoder_init_state\n",
    ")\n",
    "\n",
    "attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name=\"concat\")(\n",
    "    [decoder_inf_out, attn_inf_out]\n",
    ")\n",
    "decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "\n",
    "decoder_model = Model(\n",
    "    inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "    outputs=[\n",
    "        decoder_inf_pred,\n",
    "        attn_inf_states,\n",
    "        decoder_inf_state_h,\n",
    "        decoder_inf_state_c,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Saved HDF5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Set these parameters (embedded in file name)\n",
    "latent_dim = 64\n",
    "\n",
    "loaded_model = load_model(\n",
    "    f\"{config.MODELS_DIR}{simple_name}-N({len(X_train)})-{latent_dim}.best.h5\",\n",
    "    custom_objects={\"AttentionLayer\": AttentionLayer},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "# encoder = loaded_model.layers[2]\n",
    "# encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# encoder_model = Model(encoder_inputs, [state_h, state_c])\n",
    "\n",
    "# decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# decoder_dense = loaded_model.layers[4]\n",
    "\n",
    "# decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "# decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "# decoder_lstm = loaded_model.layers[3]\n",
    "# decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "#     decoder_inputs, initial_state=decoder_states_inputs\n",
    "# )\n",
    "# decoder_states = [state_h, state_c]\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "# decoder_model = Model(\n",
    "#     [decoder_inputs] + decoder_states_inputs, \n",
    "#     [decoder_outputs] + decoder_states\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-GRU + Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_inf_inputs = Input(shape=(None, num_encoder_tokens), name=\"encoder_inf_inputs\")\n",
    "# encoder = full_model.layers[1]\n",
    "# encoder_inf_out, encoder_inf_fwd_state, encoder_inf_back_state = encoder(\n",
    "#     encoder_inf_inputs\n",
    "# )\n",
    "# encoder_model = Model(\n",
    "#     inputs=encoder_inf_inputs,\n",
    "#     outputs=[encoder_inf_out, encoder_inf_fwd_state, encoder_inf_back_state],\n",
    "# )\n",
    "\n",
    "# \"\"\" Decoder (Inference) model \"\"\"\n",
    "# decoder_inf_inputs = Input(shape=(None, num_decoder_tokens), name=\"decoder_inf_inputs\")\n",
    "# encoder_inf_states = Input(\n",
    "#     batch_shape=(None, None, 2 * latent_dim), name=\"encoder_inf_states\"\n",
    "# )\n",
    "# decoder_init_state = Input(batch_shape=(None, 2 * latent_dim), name=\"decoder_init\")\n",
    "\n",
    "# decoder = full_model.layers[4]\n",
    "# dense = full_model.layers[7]\n",
    "# decoder_inf_out, decoder_inf_state = decoder(\n",
    "#     decoder_inf_inputs, initial_state=decoder_init_state\n",
    "# )\n",
    "# attn_layer = full_model.layers[5]\n",
    "# attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "# decoder_inf_concat = Concatenate(axis=-1, name=\"concat\")(\n",
    "#     [decoder_inf_out, attn_inf_out]\n",
    "# )\n",
    "# decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "# decoder_model = Model(\n",
    "#     inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "#     outputs=[decoder_inf_pred, attn_inf_states, decoder_inf_state],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_test = [\"he likes cats and dogs\",\"we do not know anything\"]\n",
    "\n",
    "# encoder_test_data = np.zeros(\n",
    "#     (len(X_test), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    "# )\n",
    "# for i, x in enumerate(X_test):\n",
    "#     for t, char in enumerate(x):\n",
    "#         encoder_test_data[i, t, input_token_index[char]] = 1.0\n",
    "#     encoder_test_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "\n",
    "# import sys\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# correct = 0\n",
    "# checked = 0\n",
    "# for seq_index in range(0, len(X_test)):\n",
    "#     test_X = X_test[seq_index]\n",
    "#     input_seq = encoder_test_data[seq_index : seq_index + 1]\n",
    "\n",
    "#     # Bi-LSTM\n",
    "#     decoded_sentence, attn_weights = decode_sequence_bilstm_attn(\n",
    "#         encoder_model, decoder_model, input_seq, num_encoder_tokens, num_decoder_tokens\n",
    "#     )\n",
    "\n",
    "#     # LSTM\n",
    "#     # decoded_sentence = decode_sequence_lstm(input_seq)\n",
    "#     plot_attention_weights(\n",
    "#         input_seq,\n",
    "#         attn_weights,\n",
    "#         reverse_input_char_index,\n",
    "#         reverse_target_char_index,\n",
    "#         filename=\"attention_{}.png\".format(seq_index),\n",
    "#     )\n",
    "#     print(\"-\")\n",
    "#     print(\"Input sentence:\", X_test[seq_index])\n",
    "#     print(\"Decoded sentence:\", repr(decoded_sentence.rstrip()))\n",
    "#     print(\"Real sentence:\", repr(y_test[seq_index]))\n",
    "#     print(\"CORRECT\" if decoded_sentence.rstrip() == y_test[seq_index] else \"INCORRECT\")\n",
    "#     correct += 1 if decoded_sentence.rstrip() == y_test[seq_index] else 0\n",
    "#     checked += 1\n",
    "#     print(f\"Completed: {(checked / len(X_test)) * 100}%\")\n",
    "#     print(f\"{checked}/{len(X_test)}\")\n",
    "#     print(\"Accuracy:\")\n",
    "#     print(f\"{(correct / checked) * 100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def test_func(line):\n",
    "    X_test = [line]\n",
    "\n",
    "    encoder_test_data = np.zeros(\n",
    "        (len(X_test), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    "    )\n",
    "    for i, x in enumerate(X_test):\n",
    "        for t, char in enumerate(x):\n",
    "            encoder_test_data[i, t, input_token_index[char]] = 1.0\n",
    "        encoder_test_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "\n",
    "    # np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "    correct = 0\n",
    "    checked = 0\n",
    "    dec_line = \"\"\n",
    "    for seq_index in range(0, len(X_test)):\n",
    "        test_X = X_test[seq_index]\n",
    "        input_seq = encoder_test_data[seq_index : seq_index + 1]\n",
    "\n",
    "        # Bi-LSTM\n",
    "        decoded_sentence, attn_weights = decode_sequence_bilstm_attn(\n",
    "            encoder_model, decoder_model, input_seq, num_encoder_tokens, num_decoder_tokens\n",
    "        )\n",
    "\n",
    "        # LSTM\n",
    "        # decoded_sentence = decode_sequence_lstm(input_seq)\n",
    "        # plot_attention_weights(\n",
    "        #     input_seq,\n",
    "        #     attn_weights,\n",
    "        #     reverse_input_char_index,\n",
    "        #     reverse_target_char_index,\n",
    "        #     filename=\"attention_{}.png\".format(seq_index),\n",
    "        # )\n",
    "        print(\"-\")\n",
    "        origin_input = X_test[seq_index]\n",
    "        print(\"Input sentence:\", origin_input)\n",
    "        print(\"Decoded sentence:\", repr(decoded_sentence.rstrip()))\n",
    "        dec_line = repr(decoded_sentence.rstrip())\n",
    "        w1=dec_line.split(\"_\")[1].split(\"(\")[0]\n",
    "        w2=dec_line.split(\"_\")[2].split(\"(\")[0]\n",
    "        if origin_input.find(\" \"+w1+\" \")!=-1 or origin_input.find(\" \"+w2+\" \")!=-1:\n",
    "            # print(origin_input)\n",
    "            return True, dec_line\n",
    "        else:\n",
    "            # print(origin_input)\n",
    "            return False, dec_line\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TARGET_FILE = \"../../clean_text/Windows_2k.log_clean.txt\"\n",
    "TARGET_FILE = \"../../clean_text/Windows_FALSE.txt\"\n",
    "\n",
    "file = open(TARGET_FILE,\"r\")\n",
    "lines = file.readlines()\n",
    "result_file = open(\"test_result.txt\",\"w\")\n",
    "\n",
    "true_cnt = 2\n",
    "false_cnt = 2645\n",
    "\n",
    "for line in lines:\n",
    "    target_line = line.strip()\n",
    "    func_res = test_func(target_line)\n",
    "    if func_res[0]:\n",
    "        true_cnt +=1\n",
    "        result_file.write(f\"TRUE \"+str(true_cnt)+\" : \")\n",
    "        result_file.write(target_line+\" | \"+ str(func_res) +\"\\n\")\n",
    "    else:\n",
    "        false_cnt +=1\n",
    "        result_file.write(f\"FALSE \"+str(false_cnt)+\" : \")\n",
    "        result_file.write(target_line+\" | \"+ str(func_res) +\"\\n\")\n",
    "\n",
    "result_file.write(\"\\n\"+str(true_cnt) +\"/\"+ str(false_cnt) +\"\\n\")\n",
    "result_file.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "80ae40b66a50a4a015ad52259453a40d6b5351da7a6cd9ee122e050d6ff8c162"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
