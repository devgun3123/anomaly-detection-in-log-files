{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.15\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/49/91rbcx2d7k15zt7fz8y75q9r0000gn/T/ipykernel_29664/3326022288.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Config:\n",
    "    PROJECT_DIR = os.environ[\"PWD\"]\n",
    "    DATA_DIR = os.getenv(\"DATA_DIR\", \"data/\")\n",
    "    RESULTS_DIR = os.getenv(\"RESULTS_DIR\", \"results/\")\n",
    "    MODELS_DIR = os.getenv(\"MODELS_DIR\", \"models/\")\n",
    "    CHECKPOINT_DIR = os.getenv(\"CHECKPOINT_DIR\", \"models/checkpoint/\")\n",
    "    LOGS_DIR = os.getenv(\"LOGS_DIR\", \"logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(X, y, num_s, num_e, ratio):\n",
    "    print('Stats:')\n",
    "    print(\"------------------------\")\n",
    "    print(\"------------------------\")\n",
    "    print(f'N(X) == N(y) == {len(y)}')\n",
    "    print(f'errs: {num_e}')\n",
    "    print(f'Clean data (N = {num_s}) ratio: {ratio}%')\n",
    "    print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "X_train_dir = f\"{config.DATA_DIR}clean/\"\n",
    "y_train_dir = f\"{config.DATA_DIR}trans/\"\n",
    "X_test_dir = f\"{config.DATA_DIR}test/\"\n",
    "\n",
    "# Will notify if these values change\n",
    "max_encoder_seq_length = 81\n",
    "max_decoder_seq_length = 162"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "# All of the characters and substring that would mark lines in the training data as \"faulty\"\n",
    "invalid_chars = set(\n",
    "    [\n",
    "        \":\",\n",
    "        \"+\",\n",
    "        \"#\",\n",
    "        \"@\",\n",
    "        \"Ö\",\n",
    "        \"á\",\n",
    "        \"ä\",\n",
    "        \"é\",\n",
    "        \"í\",\n",
    "        \"ñ\",\n",
    "        \"ó\",\n",
    "        \"ö\",\n",
    "        \"ú\",\n",
    "        \"ā\",\n",
    "        \"Ć\",\n",
    "        \"ć\",\n",
    "        \"ʻ\",\n",
    "        \"́\",\n",
    "        \"е\",\n",
    "        \"н\",\n",
    "        \"о\",\n",
    "        \"п\",\n",
    "        \"у\",\n",
    "        \"ш\",\n",
    "        \"ã\",\n",
    "        \"ï\",\n",
    "        \"ō\",\n",
    "        \"ū\",\n",
    "        \"ί\",\n",
    "        \"α\",\n",
    "        \"δ\",\n",
    "        \"ε\",\n",
    "        \"κ\",\n",
    "        \"ο\",\n",
    "        \"в\",\n",
    "        \"ὐ\",\n",
    "        chr(776),\n",
    "        \"ç\",\n",
    "        \"ē\",\n",
    "        \"D\",\n",
    "        \"O\",\n",
    "        \"T\",\n",
    "    ]\n",
    ")\n",
    "invalid_chars_X = set([\"(\", \")\", \"<\", \">\", \"_\", \",\"])\n",
    "invalid_markers = set([\"\\\\F\", \"TrueP\", \"\\\\x\", \"semantics_error\", \"Prog(\"])\n",
    "files_with_compound_preds = [20, 21, 15]\n",
    "\n",
    "\n",
    "def mark_if_faulty(line, file_idx, X=False):\n",
    "    if X and (\n",
    "        any((c in invalid_chars) for c in line)\n",
    "        or any((c in invalid_chars_X) for c in line)\n",
    "    ):\n",
    "        return \"syntax_error\"\n",
    "    # TODO: Refactor this hacky workaround\n",
    "    if line[0] == \"(\" and file_idx not in files_with_compound_preds:\n",
    "        return \"syntax_error\"\n",
    "    if any((m in line) for m in invalid_markers) or any(\n",
    "        (c in invalid_chars) for c in line\n",
    "    ):\n",
    "        return \"syntax_error\"\n",
    "    # Remove top-level parentheses from lambda expression\n",
    "    if line[0:4] == \"(exi\" and line[-1] == \")\":\n",
    "        line = line[1:-1]\n",
    "    if line[0:4] == \"(all\" and line[-1] == \")\":\n",
    "        line = line[1:-1]\n",
    "\n",
    "    return line\n",
    "\n",
    "\n",
    "def lines_from_file(direc, name, drop_punc=False, lower=True, drop_fullstop=True):\n",
    "    with open(direc + name) as f:\n",
    "        for l in f:\n",
    "            l = l.rstrip()\n",
    "            if drop_punc:\n",
    "                l = l.translate(table)\n",
    "            if lower:\n",
    "                l = l.lower()\n",
    "            if drop_fullstop and not drop_punc:\n",
    "                l = l[0:-1]\n",
    "            yield l\n",
    "\n",
    "\n",
    "def load_and_clean_data(start_idx=1, end_idx=17, skip_idx_list=None):\n",
    "    X, y = [], []\n",
    "\n",
    "    err = lambda x: x == \"syntax_error\"\n",
    "    X_name = lambda i: f\"concordance_{i}_clean.txt\"\n",
    "    y_name = lambda i: f\"concordance_{i}_clean.lam\"\n",
    "\n",
    "    # Load lines from files and mark those that are \"faulty\"\n",
    "    for i in range(start_idx, end_idx + 1):\n",
    "        if i in skip_idx_list:\n",
    "            continue\n",
    "\n",
    "        X = X + [\n",
    "            mark_if_faulty(line, i, True)\n",
    "            for line in lines_from_file(X_train_dir, X_name(i), drop_fullstop=True)\n",
    "        ]\n",
    "        y = y + [\n",
    "            mark_if_faulty(line, i)\n",
    "            for line in lines_from_file(\n",
    "                y_train_dir, y_name(i), lower=False, drop_fullstop=False\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Save \"faulty\" line indices\n",
    "    err_idx_X = [i1 for i1 in range(len(X)) if err(X[i1])]\n",
    "    err_idx_y = [j1 for j1 in range(len(X)) if err(y[j1])]\n",
    "\n",
    "    err_idx = set(err_idx_X).union(set(err_idx_y))\n",
    "    num_err = len(err_idx)\n",
    "    num_samples = len(y) - num_err\n",
    "    clean_ratio = 100 - ((num_err / len(y)) * 100)\n",
    "\n",
    "    # Show stats about training data\n",
    "    print_stats(X, y, num_samples, num_err, clean_ratio)\n",
    "\n",
    "    # Remove \"faulty\" lines\n",
    "    for index in sorted(list(err_idx), reverse=True):\n",
    "        del X[index]\n",
    "        del y[index]\n",
    "\n",
    "    return (X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats:\n",
      "------------------------\n",
      "------------------------\n",
      "N(X) == N(y) == 40000\n",
      "errs: 0\n",
      "Clean data (N = 40000) ratio: 100.0%\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    *load_and_clean_data(1, 20, [ ]), test_size=0.25, random_state=4, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 30000\n",
      "Number of unique input tokens: 37\n",
      "Number of unique output tokens: 45\n",
      "WARNING: NEW Max sequence length for inputs: 164\n",
      "Dataset may be incompatible with older models.\n",
      "['\\t', '\\n', ' ', '&', '(', ')', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "for i in range(0, len(X_train)):\n",
    "    # SOS == '\\n'\n",
    "    # EOS == '\\t'\n",
    "    y_train[i] = \"\\t\" + y_train[i] + \"\\n\"\n",
    "\n",
    "    for char in X_train[i]:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in y_train[i]:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_X_len = max([len(txt) for txt in X_train])\n",
    "max_y_len = max([len(txt) for txt in y_train])\n",
    "\n",
    "print(\"Number of samples:\", len(X_train))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "\n",
    "if max_X_len > max_encoder_seq_length:\n",
    "    print(\"WARNING: NEW Max sequence length for inputs:\", max_X_len)\n",
    "    print(\"Dataset may be incompatible with older models.\")\n",
    "    max_encoder_seq_length = max_X_len\n",
    "\n",
    "if max_y_len > max_decoder_seq_length:\n",
    "    print(\"WARNING: NEW Max sequence length for outputs:\", max_y_len)\n",
    "    print(\"Dataset may be incompatible with older models.\")\n",
    "    max_decoder_seq_length = max_y_len\n",
    "\n",
    "print(target_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# char to index\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(X_train), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(X_train), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(X_train), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "np.set_printoptions(threshold=10)\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(X_train, y_train)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0\n",
    "\n",
    "encoder_input_data[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(\n",
    "            name=\"W_a\",\n",
    "            shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.U_a = self.add_weight(\n",
    "            name=\"U_a\",\n",
    "            shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.V_a = self.add_weight(\n",
    "            name=\"V_a\",\n",
    "            shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "            initializer=\"uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        super(AttentionLayer, self).build(\n",
    "            input_shape\n",
    "        )  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs, verbose=False):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "        if verbose:\n",
    "            print(\"encoder_out_seq>\", encoder_out_seq.shape)\n",
    "            print(\"decoder_out_seq>\", decoder_out_seq.shape)\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\"Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(\n",
    "                states, type(states)\n",
    "            )\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
    "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
    "            de_hidden = inputs.shape[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(\n",
    "                K.dot(inputs, self.U_a), 1\n",
    "            )  # <= batch_size, 1, latent_dim\n",
    "            if verbose:\n",
    "                print(\"Ua.h>\", U_a_dot_h.shape)\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "            if verbose:\n",
    "                print(\"Ws+Uh>\", Ws_plus_Uh.shape)\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            if verbose:\n",
    "                print(\"ei>\", e_i.shape)\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(\n",
    "                states, type(states)\n",
    "            )\n",
    "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "            if verbose:\n",
    "                print(\"ci>\", c_i.shape)\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(\n",
    "            encoder_out_seq, axis=2\n",
    "        )  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step,\n",
    "            decoder_out_seq,\n",
    "            [fake_state_e],\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step,\n",
    "            e_outputs,\n",
    "            [fake_state_c],\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1])),\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Input,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Dense,\n",
    "    Concatenate,\n",
    "    TimeDistributed,\n",
    "    Bidirectional,\n",
    ")\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from livelossplot import PlotLossesKeras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla LSTM (Sutskever et al.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 300  # Number of epochs to train for.\n",
    "simple_name = \"lstm\"\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(\n",
    "    latent_dim, recurrent_dropout=0.1, return_state=True, name=\"encoder\"\n",
    ")\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(\n",
    "    latent_dim,\n",
    "    recurrent_dropout=0.1,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    name=\"decoder\",\n",
    ")\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_pred = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "\n",
    "full_model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 96\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 300  # Number of epochs to train for.\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = GRU(latent_dim, recurrent_dropout=0.333, return_state=True, name=\"encoder\")\n",
    "encoder_outputs, encoder_state = encoder(encoder_inputs)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_gru = GRU(\n",
    "    latent_dim,\n",
    "    recurrent_dropout=0.2,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    name=\"decoder\",\n",
    ")\n",
    "decoder_outputs, _ = decoder_gru(decoder_inputs, initial_state=encoder_state)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_pred = decoder_dense(decoder_outputs)\n",
    "\n",
    "early_stop = EarlyStopping(patience=3, monitor=\"val_loss\")\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "\n",
    "full_model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional GRU + Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 48\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 30  # Number of epochs to train for.\n",
    "recurrent_dropout_rate = 0.2\n",
    "dropout_rate = 0.5\n",
    "simple_name = \"bigru\"\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# Encoder GRU\n",
    "encoder_gru = Bidirectional(\n",
    "    GRU(\n",
    "        latent_dim,\n",
    "        return_sequences=True,\n",
    "        return_state=True,\n",
    "        name=\"encoder_gru\",\n",
    "        recurrent_dropout=recurrent_dropout_rate,\n",
    "    ),\n",
    "    name=\"bidirectional_encoder\",\n",
    ")\n",
    "encoder_out, encoder_fwd_state, encoder_back_state = encoder_gru(encoder_inputs)\n",
    "\n",
    "# Set up the decoder GRU, using `encoder_states` as initial state.\n",
    "decoder_gru = GRU(\n",
    "    latent_dim * 2,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    name=\"decoder_gru\",\n",
    "    recurrent_dropout=recurrent_dropout_rate,\n",
    ")\n",
    "decoder_out, decoder_state = decoder_gru(\n",
    "    decoder_inputs,\n",
    "    initial_state=Concatenate(axis=-1)([encoder_fwd_state, encoder_back_state]),\n",
    ")\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name=\"attention_layer\")\n",
    "attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "# Concat attention input and decoder GRU output\n",
    "decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")(\n",
    "    [decoder_out, attn_out]\n",
    ")\n",
    "\n",
    "# Dense layer\n",
    "dense = Dense(num_decoder_tokens, activation=\"softmax\", name=\"softmax_layer\")\n",
    "decoder_pred = dense(decoder_concat_input)\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "# opt = Adam(\n",
    "#     learning_rate=0.001,\n",
    "#     beta_1=0.9,\n",
    "#     beta_2=0.999,\n",
    "#     epsilon=1e-07,\n",
    "#     amsgrad=True,\n",
    "#     name='Adam'\n",
    "# )\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.0015,\n",
    "    rho=0.9,\n",
    "    momentum=0.0,\n",
    "    epsilon=1e-07,\n",
    "    centered=True,\n",
    "    name=\"RMSprop\",\n",
    ")\n",
    "\n",
    "# Full model\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "full_model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM + Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 44\n",
    "batch_size = 48  # Batch size for training.\n",
    "epochs = 30  # Number of epochs to train for.\n",
    "recurrent_dropout_rate = 0.2\n",
    "simple_name = \"bilstm\"\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# Encoder LSTM\n",
    "encoder_lstm = Bidirectional(\n",
    "    LSTM(\n",
    "        latent_dim,\n",
    "        return_sequences=True,\n",
    "        return_state=True,\n",
    "        name=\"encoder_lstm\",\n",
    "        recurrent_dropout=recurrent_dropout_rate,\n",
    "    ),\n",
    "    name=\"bidirectional_encoder\",\n",
    ")\n",
    "(\n",
    "    encoder_out,\n",
    "    encoder_fwd_state_h,\n",
    "    encoder_fwd_state_c,\n",
    "    encoder_back_state_h,\n",
    "    encoder_back_state_c,\n",
    ") = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(\n",
    "    latent_dim * 2,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_dropout=recurrent_dropout_rate,\n",
    "    name=\"decoder_lstm\",\n",
    ")\n",
    "decoder_out, _, _ = decoder_lstm(\n",
    "    decoder_inputs,\n",
    "    initial_state=[\n",
    "        Concatenate(axis=-1)([encoder_fwd_state_h, encoder_back_state_h]),\n",
    "        Concatenate(axis=-1)([encoder_fwd_state_c, encoder_back_state_c]),\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Attention layer\n",
    "attn_layer = AttentionLayer(name=\"attention_layer\")\n",
    "attn_out, attn_states = attn_layer([encoder_out, decoder_out])\n",
    "\n",
    "# Concat attention input and decoder LSTM output\n",
    "decoder_concat_input = Concatenate(axis=-1, name=\"concat_layer\")(\n",
    "    [decoder_out, attn_out]\n",
    ")\n",
    "\n",
    "# Dense layer\n",
    "dense = Dense(num_decoder_tokens, activation=\"softmax\", name=\"softmax_layer\")\n",
    "decoder_pred = dense(decoder_concat_input)\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "# opt = Adam(\n",
    "#     learning_rate=0.001,\n",
    "#     beta_1=0.9,\n",
    "#     beta_2=0.999,\n",
    "#     epsilon=1e-07,\n",
    "#     amsgrad=True,\n",
    "#     name=\"Adam\",\n",
    "# )\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(\n",
    "    learning_rate=0.0015,\n",
    "    rho=0.9,\n",
    "    momentum=0.0,\n",
    "    epsilon=1e-07,\n",
    "    centered=True,\n",
    "    name=\"RMSprop\",\n",
    ")\n",
    "\n",
    "# Full model\n",
    "full_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_pred)\n",
    "full_model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, None, 37)]   0           []                               \n",
      "                                                                                                  \n",
      " bidirectional_encoder (Bidirec  [(None, None, 88),  28864       ['input_7[0][0]']                \n",
      " tional)                         (None, 44),                                                      \n",
      "                                 (None, 44),                                                      \n",
      "                                 (None, 44),                                                      \n",
      "                                 (None, 44)]                                                      \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None, None, 45)]   0           []                               \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 88)           0           ['bidirectional_encoder[0][1]',  \n",
      "                                                                  'bidirectional_encoder[0][3]']  \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate)    (None, 88)           0           ['bidirectional_encoder[0][2]',  \n",
      "                                                                  'bidirectional_encoder[0][4]']  \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)            [(None, None, 88),   47168       ['input_9[0][0]',                \n",
      "                                 (None, 88),                      'concatenate_1[0][0]',          \n",
      "                                 (None, 88)]                      'concatenate_2[0][0]']          \n",
      "                                                                                                  \n",
      " attention_layer (AttentionLaye  ((None, None, 88),  15576       ['bidirectional_encoder[0][0]',  \n",
      " r)                              (None, None, None)               'decoder_lstm[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " concat_layer (Concatenate)     (None, None, 176)    0           ['decoder_lstm[0][0]',           \n",
      "                                                                  'attention_layer[0][0]']        \n",
      "                                                                                                  \n",
      " softmax_layer (Dense)          (None, None, 45)     7965        ['concat_layer[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 99,573\n",
      "Trainable params: 99,573\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "\ttraining         \t (min:    0.911, max:    0.911, cur:    0.911)\n",
      "\tvalidation       \t (min:    0.975, max:    0.975, cur:    0.975)\n",
      "Loss\n",
      "\ttraining         \t (min:    0.340, max:    0.340, cur:    0.340)\n",
      "\tvalidation       \t (min:    0.094, max:    0.094, cur:    0.094)\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 0.09426, saving model to models/checkpoint/weights-bilstm-N(30000)-44.best.hdf5\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to create file (unable to open file: name = 'models/checkpoint/weights-bilstm-N(30000)-44.best.hdf5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m checkpoint \u001b[39m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m     15\u001b[0m     cp_path, save_weights_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, monitor\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m\"\u001b[39m, save_best_only\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m \u001b[39m# Start Training\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m history \u001b[39m=\u001b[39m full_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     20\u001b[0m     [encoder_input_data, decoder_input_data],\n\u001b[1;32m     21\u001b[0m     decoder_target_data,\n\u001b[1;32m     22\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     23\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     24\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     25\u001b[0m     shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     26\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[early_stop, plot_loss, checkpoint],\n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/log-nlp_38/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/log-nlp_38/lib/python3.8/site-packages/h5py/_hl/files.py:507\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, **kwds)\u001b[0m\n\u001b[1;32m    502\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    503\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    504\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[1;32m    505\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[1;32m    506\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 507\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39mswmr)\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    510\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[0;32m~/miniforge3/envs/log-nlp_38/lib/python3.8/site-packages/h5py/_hl/files.py:226\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    224\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mcreate(name, h5f\u001b[39m.\u001b[39mACC_EXCL, fapl\u001b[39m=\u001b[39mfapl, fcpl\u001b[39m=\u001b[39mfcpl)\n\u001b[1;32m    225\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 226\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mcreate(name, h5f\u001b[39m.\u001b[39;49mACC_TRUNC, fapl\u001b[39m=\u001b[39;49mfapl, fcpl\u001b[39m=\u001b[39;49mfcpl)\n\u001b[1;32m    227\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    228\u001b[0m     \u001b[39m# Open in append mode (read/write).\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39m# If that fails, create a new file only if it won't clobber an\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[39m# existing one (ACC_EXCL)\u001b[39;00m\n\u001b[1;32m    231\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:126\u001b[0m, in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to create file (unable to open file: name = 'models/checkpoint/weights-bilstm-N(30000)-44.best.hdf5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "cp_path = f\"{config.CHECKPOINT_DIR}weights-{simple_name}-N({len(X_train)})-{latent_dim}.best.hdf5\"\n",
    "\n",
    "# Load weights, if any from previous run\n",
    "# full_model.load_weights(cp_path)\n",
    "\n",
    "# Callbacks\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "early_stop = EarlyStopping(patience=2, monitor=\"val_loss\", mode=\"min\", verbose=1)\n",
    "plot_loss = PlotLossesKeras()\n",
    "checkpoint = ModelCheckpoint(\n",
    "    cp_path, save_weights_only=True, verbose=1, monitor=\"val_loss\", save_best_only=True\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "history = full_model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    shuffle=True,\n",
    "    callbacks=[early_stop, plot_loss, checkpoint],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model.save(f\"{config.MODELS_DIR}{simple_name}-N({len(X_train)})-{latent_dim}.best.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence_bigru_attn(\n",
    "    encoder_model, decoder_model, test_X_seq, num_encoder_tokens, num_decoder_tokens\n",
    "):\n",
    "    \"\"\"\n",
    "    Infer logic\n",
    "    :param encoder_model: keras.Model\n",
    "    :param decoder_model: keras.Model\n",
    "    :param test_X_seq: sequence of word ids\n",
    "    :param num_encoder_tokens: int\n",
    "    :param num_decoder_tokens: int\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    enc_outs, enc_fwd_state, enc_back_state = encoder_model.predict(test_X_seq)\n",
    "    dec_state = np.concatenate([enc_fwd_state, enc_back_state], axis=-1)\n",
    "\n",
    "    attention_weights = []\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        dec_out, attention, dec_state = decoder_model.predict(\n",
    "            [enc_outs, dec_state, target_seq]\n",
    "        )\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(dec_out, axis=-1)[0, 0]\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        attention_weights.append((sampled_token_index, attention))\n",
    "\n",
    "    return decoded_sentence, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence_bilstm_attn(\n",
    "    encoder_model, decoder_model, test_X_seq, num_encoder_tokens, num_decoder_tokens\n",
    "):\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    (\n",
    "        enc_outs,\n",
    "        enc_fwd_state_h,\n",
    "        enc_fwd_state_c,\n",
    "        enc_back_state_h,\n",
    "        enc_back_state_c,\n",
    "    ) = encoder_model.predict(test_X_seq)\n",
    "    encoder_state_h = np.concatenate([enc_fwd_state_h, enc_back_state_h], axis=-1)\n",
    "    encoder_state_c = np.concatenate([enc_fwd_state_c, enc_back_state_c], axis=-1)\n",
    "\n",
    "    # The ordering seems significant\n",
    "    # enc_outs, enc_fwd_state_h, enc_fwd_state_c, enc_back_state_h, enc_back_state_c = encoder_model.predict(test_X_seq)\n",
    "\n",
    "    attention_weights = []\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        dec_out, attention, dec_state_h, dec_state_c = decoder_model.predict(\n",
    "            [enc_outs, encoder_state_h, encoder_state_c, target_seq]\n",
    "        )\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(dec_out[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        attention_weights.append((sampled_token_index, attention))\n",
    "\n",
    "        # Update states\n",
    "        encoder_state_h = dec_state_h\n",
    "        encoder_state_c = dec_state_c\n",
    "\n",
    "    return decoded_sentence, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence_lstm(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_weights(\n",
    "    encoder_inputs, attention_weights, en_id2word, fr_id2word, filename=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots attention weights\n",
    "    :param encoder_inputs: Sequence of word ids (list/numpy.ndarray)\n",
    "    :param attention_weights: Sequence of (<word_id_at_decode_step_t>:<attention_weights_at_decode_step_t>)\n",
    "    :param en_id2word: dict\n",
    "    :param fr_id2word: dict\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if len(attention_weights) == 0:\n",
    "        print(\n",
    "            \"Your attention weights were empty. No attention map saved to the disk. \"\n",
    "            + \"\\nPlease check if the decoder produced  a proper translation\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    mats = []\n",
    "    dec_inputs = []\n",
    "    for dec_ind, attn in attention_weights:\n",
    "        mats.append(attn.reshape(-1))\n",
    "        dec_inputs.append(dec_ind)\n",
    "    attention_mat = np.transpose(np.array(mats))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(32, 32))\n",
    "    ax.imshow(attention_mat)\n",
    "\n",
    "    ax.set_xticks(np.arange(attention_mat.shape[1]))\n",
    "    ax.set_yticks(np.arange(attention_mat.shape[0]))\n",
    "\n",
    "    ax.set_xticklabels([fr_id2word[inp] if inp != 0 else \"<Res>\" for inp in dec_inputs])\n",
    "    y_lab = [\n",
    "        en_id2word[inp] if inp != 0 else \"<Res>\"\n",
    "        for inp in [\n",
    "            np.argmax(np.squeeze(encoder_inputs)[i])\n",
    "            for i in range(0, np.squeeze(encoder_inputs).shape[0])\n",
    "        ]\n",
    "    ]\n",
    "    ax.set_yticklabels(y_lab)\n",
    "\n",
    "    ax.tick_params(labelsize=16)\n",
    "    ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "\n",
    "    if not os.path.exists(config.RESULTS_DIR):\n",
    "        os.mkdir(config.RESULTS_DIR)\n",
    "    if filename is None:\n",
    "        plt.savefig(os.path.join(config.RESULTS_DIR, \"attention.png\"))\n",
    "    else:\n",
    "        plt.savefig(os.path.join(config.RESULTS_DIR, \"{}\".format(filename)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 44\n",
    "\n",
    "full_model.load_weights(\n",
    "    f\"{config.CHECKPOINT_DIR}weights-{simple_name}-N({len(X_train)})-{latent_dim}.best.hdf5\"\n",
    ")\n",
    "loaded_model = full_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-LSTM + Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Encoder (Inference) model \"\"\"\n",
    "encoder_inf_inputs = Input(shape=(None, num_encoder_tokens), name=\"encoder_inf_inputs\")\n",
    "(\n",
    "    encoder_inf_out,\n",
    "    encoder_inf_fwd_state_h,\n",
    "    encoder_inf_fwd_state_c,\n",
    "    encoder_inf_back_state_h,\n",
    "    encoder_inf_back_state_c,\n",
    ") = encoder_lstm(encoder_inf_inputs)\n",
    "encoder_model = Model(\n",
    "    inputs=encoder_inf_inputs,\n",
    "    outputs=[\n",
    "        encoder_inf_out,\n",
    "        encoder_inf_fwd_state_h,\n",
    "        encoder_inf_fwd_state_c,\n",
    "        encoder_inf_back_state_h,\n",
    "        encoder_inf_back_state_c,\n",
    "    ],\n",
    ")\n",
    "\n",
    "\"\"\" Decoder (Inference) model \"\"\"\n",
    "decoder_inf_inputs = Input(shape=(None, num_decoder_tokens), name=\"decoder_inf_inputs\")\n",
    "encoder_inf_states = Input(\n",
    "    batch_shape=(None, None, 2 * latent_dim), name=\"encoder_inf_states\"\n",
    ")\n",
    "\n",
    "decoder_state_input_h = Input(batch_shape=(None, 2 * latent_dim))\n",
    "decoder_state_input_c = Input(batch_shape=(None, 2 * latent_dim))\n",
    "\n",
    "decoder_init_state = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_inf_out, decoder_inf_state_h, decoder_inf_state_c = decoder_lstm(\n",
    "    decoder_inf_inputs, initial_state=decoder_init_state\n",
    ")\n",
    "\n",
    "attn_inf_out, attn_inf_states = attn_layer([encoder_inf_states, decoder_inf_out])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name=\"concat\")(\n",
    "    [decoder_inf_out, attn_inf_out]\n",
    ")\n",
    "decoder_inf_pred = TimeDistributed(dense)(decoder_inf_concat)\n",
    "\n",
    "decoder_model = Model(\n",
    "    inputs=[encoder_inf_states, decoder_init_state, decoder_inf_inputs],\n",
    "    outputs=[\n",
    "        decoder_inf_pred,\n",
    "        attn_inf_states,\n",
    "        decoder_inf_state_h,\n",
    "        decoder_inf_state_c,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Saved HDF5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# # Set these parameters (embedded in file name)\n",
    "# latent_dim = 64\n",
    "\n",
    "# loaded_model = load_model(\n",
    "#     f\"{config.MODELS_DIR}{simple_name}-N({len(X_train)})-{latent_dim}.best.h5\",\n",
    "#     custom_objects={\"AttentionLayer\": AttentionLayer},\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loaded_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Test Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_test = [\"he likes cats and dogs\",\"we do not know anything\"]\n",
    "\n",
    "# encoder_test_data = np.zeros(\n",
    "#     (len(X_test), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    "# )\n",
    "# for i, x in enumerate(X_test):\n",
    "#     for t, char in enumerate(x):\n",
    "#         encoder_test_data[i, t, input_token_index[char]] = 1.0\n",
    "#     encoder_test_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "\n",
    "# import sys\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "# correct = 0\n",
    "# checked = 0\n",
    "# for seq_index in range(0, len(X_test)):\n",
    "#     test_X = X_test[seq_index]\n",
    "#     input_seq = encoder_test_data[seq_index : seq_index + 1]\n",
    "\n",
    "#     # Bi-LSTM\n",
    "#     decoded_sentence, attn_weights = decode_sequence_bilstm_attn(\n",
    "#         encoder_model, decoder_model, input_seq, num_encoder_tokens, num_decoder_tokens\n",
    "#     )\n",
    "\n",
    "#     # LSTM\n",
    "#     # decoded_sentence = decode_sequence_lstm(input_seq)\n",
    "#     plot_attention_weights(\n",
    "#         input_seq,\n",
    "#         attn_weights,\n",
    "#         reverse_input_char_index,\n",
    "#         reverse_target_char_index,\n",
    "#         filename=\"attention_{}.png\".format(seq_index),\n",
    "#     )\n",
    "#     print(\"-\")\n",
    "#     print(\"Input sentence:\", X_test[seq_index])\n",
    "#     print(\"Decoded sentence:\", repr(decoded_sentence.rstrip()))\n",
    "#     print(\"Real sentence:\", repr(y_test[seq_index]))\n",
    "#     print(\"CORRECT\" if decoded_sentence.rstrip() == y_test[seq_index] else \"INCORRECT\")\n",
    "#     correct += 1 if decoded_sentence.rstrip() == y_test[seq_index] else 0\n",
    "#     checked += 1\n",
    "#     print(f\"Completed: {(checked / len(X_test)) * 100}%\")\n",
    "#     print(f\"{checked}/{len(X_test)}\")\n",
    "#     print(\"Accuracy:\")\n",
    "#     print(f\"{(correct / checked) * 100}%\")\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m49",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m49"
  },
  "kernelspec": {
   "display_name": "log-nlp_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 (default, Nov 24 2022, 08:57:44) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "66d4fb87ddb4494270580ee563fb6840ad68ac8d4139104e541e8f4c217ec4ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
